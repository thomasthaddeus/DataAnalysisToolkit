{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Additional Exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Exercise 1: Integrating Multiple Data Sources\n",
    "- **Objective**: Combine data from an Excel file, a SQL database, and a JSON API into a single DataFrame.\n",
    "- **Tasks**:\n",
    "  - Use the Excel Connector to load data from an Excel file.\n",
    "  - Fetch data from a SQL database using the SQL Connector.\n",
    "  - Retrieve data from a JSON API using the API Connector.\n",
    "  - Integrate all these datasets using the Data Integrator.\n",
    "- **Challenge**: Ensure that the integrated dataset is properly aligned and handle any inconsistencies in data formats or missing values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 1: Use the Excel Connector to load data from an Excel file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataanalysistoolkit.data_sources import ExcelConnector, SQLConnector, APIConnector\n",
    "from dataanalysistoolkit.integrators import DataIntegrator\n",
    "\n",
    "# Load data from the provided Excel file\n",
    "excel_connector = ExcelConnector('/mnt/data/example_data.xlsx')\n",
    "df_excel = excel_connector.load_data(sheet_name='Sheet1')\n",
    "print(\"Data from Excel:\")\n",
    "print(df_excel.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 2: Fetch data from a SQL database using the SQL Connector.\n",
    "\n",
    "For this step, we'll assume you have a PostgreSQL database set up with a table named `customer_data`. If you need to adjust the connection details or table name, you can do so in the code below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace with your actual database URI\n",
    "sql_connector = SQLConnector('postgresql://username:password@localhost:5432/mydatabase')\n",
    "\n",
    "# Executing a SQL query to fetch data\n",
    "query = \"SELECT * FROM customer_data LIMIT 5\"\n",
    "df_sql = sql_connector.query_data(query)\n",
    "print(\"Data from SQL Database:\")\n",
    "print(df_sql.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 3: Retrieve data from a JSON API using the API Connector.\n",
    "\n",
    "For demonstration purposes, we'll assume you have access to an API endpoint. You can replace the URL and authentication details as needed.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace with the actual API base URL and authentication credentials\n",
    "api_connector = APIConnector('https://api.example.com', auth=('username', 'password'))\n",
    "\n",
    "# Fetching data from a specific endpoint\n",
    "endpoint = 'data_endpoint'\n",
    "response = api_connector.get(endpoint)\n",
    "\n",
    "# Assuming the response is JSON and converting it to a DataFrame\n",
    "df_api = pd.DataFrame(response.json())\n",
    "print(\"Data from API:\")\n",
    "print(df_api.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 4: Integrate all these datasets using the Data Integrator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the Data Integrator\n",
    "integrator = DataIntegrator()\n",
    "\n",
    "# Adding dataframes to the integrator\n",
    "integrator.add_data(df_excel)\n",
    "integrator.add_data(df_sql)\n",
    "integrator.add_data(df_api)\n",
    "\n",
    "# Assuming we want to concatenate the dataframes\n",
    "combined_df = integrator.concatenate_data()\n",
    "print(\"Combined Data:\")\n",
    "print(combined_df.head())\n",
    "\n",
    "# Alternatively, if you need to merge the dataframes on a common key:\n",
    "# combined_df = integrator.merge_data(on='common_key')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Challenge\n",
    "\n",
    "Ensure that the integrated dataset is properly aligned and handle any inconsistencies in data formats or missing values.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataanalysistoolkit.preprocessor import DataFormatter\n",
    "\n",
    "# Initialize the Data Formatter\n",
    "formatter = DataFormatter(combined_df)\n",
    "\n",
    "# Standardize date formats\n",
    "formatter.standardize_dates('date_column', date_format='%Y-%m-%d')\n",
    "\n",
    "# Normalize numeric columns\n",
    "numeric_columns = ['numeric_column1', 'numeric_column2']\n",
    "formatter.normalize_numeric(numeric_columns)\n",
    "\n",
    "# Categorize columns\n",
    "category_columns = ['category_column1', 'category_column2']\n",
    "formatter.categorize_columns(category_columns)\n",
    "\n",
    "# Fill missing values\n",
    "formatter.fill_missing_values('column_with_missing_data', fill_value=0)\n",
    "\n",
    "print(\"Cleaned and Formatted Combined Data:\")\n",
    "print(combined_df.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This completes Exercise 1 in the Jupyter notebook. You have successfully integrated data from an Excel file, a SQL database, and a JSON API into a single DataFrame, then cleaned and formatted the data for further analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Exercise 2: Data Cleaning and Transformation\n",
    "- **Objective**: Clean and transform the integrated dataset from Exercise 1.\n",
    "- **Tasks**:\n",
    "  - Identify and fill missing values in the dataset.\n",
    "  - Standardize the format of any date columns.\n",
    "  - Normalize numeric columns and convert categorical columns to a standard format.\n",
    "  - Create a new column based on a custom transformation logic.\n",
    "- **Challenge**: Try to automate as much of the data cleaning process as possible, considering future data imports."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Exercise 3: Handling Large and Complex Datasets\n",
    "- **Objective**: Work with a larger and more complex dataset of your choice (e.g., a dataset from Kaggle or a public API).\n",
    "- **Tasks**:\n",
    "  - Import the dataset using the appropriate connector(s).\n",
    "  - Explore different integration techniques to handle large datasets efficiently.\n",
    "  - Perform advanced data formatting and transformation tasks tailored to the dataset's specifics.\n",
    "- **Challenge**: Optimize the data import process for speed and memory efficiency, especially if dealing with very large datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Exercise 4: Customizing the Data Import Process\n",
    "- **Objective**: Extend or customize the DataAnalysisToolkit to suit a unique data import requirement.\n",
    "- **Tasks**:\n",
    "  - Identify a specific need or limitation in the current data import process.\n",
    "  - Modify an existing connector or create a new one to address this need.\n",
    "  - Test your custom solution with relevant data sources.\n",
    "- **Challenge**: Ensure that your custom solution is robust, handles errors gracefully, and integrates well with the rest of the toolkit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Exercise 5: Real-world Application\n",
    "- **Objective**: Apply the DataAnalysisToolkit to a real-world data analysis project.\n",
    "- **Tasks**:\n",
    "  - Identify a real-world problem that can be addressed through data analysis.\n",
    "  - Collect and import data from relevant sources using the toolkit.\n",
    "  - Clean, transform, and integrate the data in preparation for analysis.\n",
    "- **Challenge**: Provide insights, visualizations, or a predictive model based on the integrated dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Sure! Let's continue with Exercise 1 by using the provided Excel file and writing out the rest of the steps in a Jupyter notebook.\n",
    "\n",
    "### Exercise 1: Integrating Multiple Data Sources\n",
    "\n",
    "**Objective:** Combine data from an Excel file, a SQL database, and a JSON API into a single DataFrame.\n",
    "\n",
    "#### Step 1: Use the Excel Connector to load data from an Excel file.\n",
    "\n",
    "```python\n",
    "from dataanalysistoolkit.data_sources import ExcelConnector, SQLConnector, APIConnector\n",
    "from dataanalysistoolkit.integrators import DataIntegrator\n",
    "\n",
    "# Load data from the provided Excel file\n",
    "excel_connector = ExcelConnector('/mnt/data/example_data.xlsx')\n",
    "df_excel = excel_connector.load_data(sheet_name='Sheet1')\n",
    "print(\"Data from Excel:\")\n",
    "print(df_excel.head())\n",
    "```\n",
    "\n",
    "#### Step 2: Fetch data from a SQL database using the SQL Connector.\n",
    "\n",
    "For this step, we'll assume you have a PostgreSQL database set up with a table named `customer_data`. If you need to adjust the connection details or table name, you can do so in the code below.\n",
    "\n",
    "```python\n",
    "# Replace with your actual database URI\n",
    "sql_connector = SQLConnector('postgresql://username:password@localhost:5432/mydatabase')\n",
    "\n",
    "# Executing a SQL query to fetch data\n",
    "query = \"SELECT * FROM customer_data LIMIT 5\"\n",
    "df_sql = sql_connector.query_data(query)\n",
    "print(\"Data from SQL Database:\")\n",
    "print(df_sql.head())\n",
    "```\n",
    "\n",
    "#### Step 3: Retrieve data from a JSON API using the API Connector.\n",
    "\n",
    "For demonstration purposes, we'll assume you have access to an API endpoint. You can replace the URL and authentication details as needed.\n",
    "\n",
    "```python\n",
    "# Replace with the actual API base URL and authentication credentials\n",
    "api_connector = APIConnector('https://api.example.com', auth=('username', 'password'))\n",
    "\n",
    "# Fetching data from a specific endpoint\n",
    "endpoint = 'data_endpoint'\n",
    "response = api_connector.get(endpoint)\n",
    "\n",
    "# Assuming the response is JSON and converting it to a DataFrame\n",
    "df_api = pd.DataFrame(response.json())\n",
    "print(\"Data from API:\")\n",
    "print(df_api.head())\n",
    "```\n",
    "\n",
    "#### Step 4: Integrate all these datasets using the Data Integrator.\n",
    "\n",
    "```python\n",
    "# Initialize the Data Integrator\n",
    "integrator = DataIntegrator()\n",
    "\n",
    "# Adding dataframes to the integrator\n",
    "integrator.add_data(df_excel)\n",
    "integrator.add_data(df_sql)\n",
    "integrator.add_data(df_api)\n",
    "\n",
    "# Assuming we want to concatenate the dataframes\n",
    "combined_df = integrator.concatenate_data()\n",
    "print(\"Combined Data:\")\n",
    "print(combined_df.head())\n",
    "\n",
    "# Alternatively, if you need to merge the dataframes on a common key:\n",
    "# combined_df = integrator.merge_data(on='common_key')\n",
    "```\n",
    "\n",
    "#### Challenge: Ensure that the integrated dataset is properly aligned and handle any inconsistencies in data formats or missing values.\n",
    "\n",
    "```python\n",
    "from dataanalysistoolkit.preprocessor import DataFormatter\n",
    "\n",
    "# Initialize the Data Formatter\n",
    "formatter = DataFormatter(combined_df)\n",
    "\n",
    "# Standardize date formats\n",
    "formatter.standardize_dates('date_column', date_format='%Y-%m-%d')\n",
    "\n",
    "# Normalize numeric columns\n",
    "numeric_columns = ['numeric_column1', 'numeric_column2']\n",
    "formatter.normalize_numeric(numeric_columns)\n",
    "\n",
    "# Categorize columns\n",
    "category_columns = ['category_column1', 'category_column2']\n",
    "formatter.categorize_columns(category_columns)\n",
    "\n",
    "# Fill missing values\n",
    "formatter.fill_missing_values('column_with_missing_data', fill_value=0)\n",
    "\n",
    "print(\"Cleaned and Formatted Combined Data:\")\n",
    "print(combined_df.head())\n",
    "```\n",
    "\n",
    "This completes Exercise 1 in the Jupyter notebook. You have successfully integrated data from an Excel file, a SQL database, and a JSON API into a single DataFrame, then cleaned and formatted the data for further analysis.\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
