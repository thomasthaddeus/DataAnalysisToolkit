{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Advanced Statistical Analysis with Python\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of Contents\n",
    "\n",
    "### Title: Advanced Statistical Analysis with Python\n",
    "\n",
    "#### 1. Introduction\n",
    "\n",
    "- Brief overview of the notebook\n",
    "- Objectives and goals\n",
    "\n",
    "#### 2. Setup and Libraries\n",
    "\n",
    "- Import necessary libraries (e.g., NumPy, SciPy, pandas, statsmodels, scikit-learn, seaborn, matplotlib)\n",
    "- Setup data (load datasets, generate synthetic data if necessary)\n",
    "\n",
    "#### 3. Descriptive Statistics\n",
    "\n",
    "- Summary statistics (mean, median, mode, variance, standard deviation)\n",
    "- Data visualization (histograms, box plots, scatter plots)\n",
    "- Correlation analysis (Pearson, Spearman)\n",
    "\n",
    "#### 4. Inferential Statistics\n",
    "\n",
    "- Hypothesis testing\n",
    "  - t-tests (one-sample, independent, paired)\n",
    "  - ANOVA (one-way, two-way)\n",
    "  - Chi-square tests\n",
    "- Confidence intervals\n",
    "\n",
    "#### 5. Regression Analysis\n",
    "\n",
    "- Simple linear regression\n",
    "- Multiple linear regression\n",
    "- Polynomial regression\n",
    "- Model evaluation (R-squared, adjusted R-squared, RMSE)\n",
    "\n",
    "#### 6. Advanced Regression Techniques\n",
    "\n",
    "- Ridge regression\n",
    "- Lasso regression\n",
    "- Elastic Net\n",
    "- Logistic regression\n",
    "\n",
    "#### 7. Time Series Analysis\n",
    "\n",
    "- Introduction to time series data\n",
    "- Decomposition of time series (trend, seasonality, residual)\n",
    "- Autocorrelation and partial autocorrelation functions\n",
    "- ARIMA and SARIMA models\n",
    "\n",
    "#### 8. Principal Component Analysis (PCA)\n",
    "\n",
    "- Introduction to PCA\n",
    "- Data standardization\n",
    "- Computing the principal components\n",
    "- Explained variance and selecting the number of components\n",
    "- Visualizing principal components\n",
    "\n",
    "#### 9. Clustering\n",
    "\n",
    "- K-means clustering\n",
    "- Hierarchical clustering\n",
    "- DBSCAN\n",
    "- Evaluating clustering results (silhouette score, Davies-Bouldin index)\n",
    "\n",
    "#### 10. Advanced Statistical Models\n",
    "\n",
    "- Generalized linear models (GLMs)\n",
    "- Survival analysis (Kaplan-Meier estimator, Cox proportional hazards model)\n",
    "- Bayesian statistics (Bayesian inference, MCMC)\n",
    "\n",
    "#### 11. Resampling Methods\n",
    "\n",
    "- Bootstrap\n",
    "- Cross-validation techniques (k-fold, stratified k-fold, leave-one-out)\n",
    "\n",
    "#### 12. Statistical Machine Learning\n",
    "\n",
    "- Decision trees\n",
    "- Random forests\n",
    "- Support vector machines (SVM)\n",
    "- Neural networks\n",
    "\n",
    "#### 13. Case Studies\n",
    "\n",
    "- Real-world datasets analysis\n",
    "- End-to-end statistical analysis project\n",
    "  - Problem definition\n",
    "  - Data preprocessing\n",
    "  - Exploratory data analysis\n",
    "  - Model building and evaluation\n",
    "  - Interpretation of results\n",
    "\n",
    "#### 14. Conclusion\n",
    "\n",
    "- Summary of key takeaways\n",
    "- Further reading and resources\n",
    "- Next steps for learning and application\n",
    "\n",
    "#### 15. References and Resources\n",
    "\n",
    "- Books, papers, and articles for further reading\n",
    "- Useful libraries and tools\n",
    "- Online courses and tutorials\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "Welcome to this notebook on advanced statistical analysis using Python. This notebook is designed to guide you through various statistical methods and techniques, providing practical examples and explanations along the way. By the end of this notebook, you will have a solid understanding of advanced statistical analysis and be able to apply these techniques to real-world datasets.\n",
    "\n",
    "### Objectives\n",
    "\n",
    "- Understand and apply descriptive and inferential statistical methods.\n",
    "- Perform regression analysis and advanced regression techniques.\n",
    "- Conduct time series analysis and decomposition.\n",
    "- Utilize principal component analysis for dimensionality reduction.\n",
    "- Implement clustering algorithms and evaluate their performance.\n",
    "- Explore advanced statistical models including generalized linear models and survival analysis.\n",
    "- Learn resampling methods such as bootstrap and cross-validation.\n",
    "- Apply statistical machine learning techniques.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Setup and Libraries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "'seaborn-darkgrid' is not a valid package style, path of style file, URL of style file, or library style name (library styles are listed in `style.available`)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\Thaddeus Maximus\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\matplotlib\\style\\core.py:137\u001b[0m, in \u001b[0;36muse\u001b[1;34m(style)\u001b[0m\n\u001b[0;32m    136\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 137\u001b[0m     style \u001b[38;5;241m=\u001b[39m \u001b[43m_rc_params_in_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstyle\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    138\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[1;32mc:\\Users\\Thaddeus Maximus\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\matplotlib\\__init__.py:879\u001b[0m, in \u001b[0;36m_rc_params_in_file\u001b[1;34m(fname, transform, fail_on_error)\u001b[0m\n\u001b[0;32m    878\u001b[0m rc_temp \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m--> 879\u001b[0m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mwith\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m_open_file_or_url\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfname\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mas\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mfd\u001b[49m\u001b[43m:\u001b[49m\n\u001b[0;32m    880\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mtry\u001b[39;49;00m\u001b[43m:\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Thaddeus Maximus\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\contextlib.py:137\u001b[0m, in \u001b[0;36m_GeneratorContextManager.__enter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    136\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 137\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mnext\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgen)\n\u001b[0;32m    138\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\Thaddeus Maximus\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\matplotlib\\__init__.py:856\u001b[0m, in \u001b[0;36m_open_file_or_url\u001b[1;34m(fname)\u001b[0m\n\u001b[0;32m    855\u001b[0m fname \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mexpanduser(fname)\n\u001b[1;32m--> 856\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(fname, encoding\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[0;32m    857\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m f\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'seaborn-darkgrid'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 23\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[38;5;66;03m# Configure visualizations\u001b[39;00m\n\u001b[0;32m     22\u001b[0m get_ipython()\u001b[38;5;241m.\u001b[39mrun_line_magic(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmatplotlib\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124minline\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m---> 23\u001b[0m \u001b[43mplt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstyle\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43muse\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mseaborn-darkgrid\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     25\u001b[0m \u001b[38;5;66;03m## Setup Data\u001b[39;00m\n\u001b[0;32m     26\u001b[0m \u001b[38;5;66;03m# Load datasets or generate synthetic data\u001b[39;00m\n\u001b[0;32m     27\u001b[0m \u001b[38;5;66;03m# For demonstration, we'll use some well-known datasets from seaborn\u001b[39;00m\n\u001b[0;32m     28\u001b[0m iris \u001b[38;5;241m=\u001b[39m sns\u001b[38;5;241m.\u001b[39mload_dataset(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124miris\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\Thaddeus Maximus\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\matplotlib\\style\\core.py:139\u001b[0m, in \u001b[0;36muse\u001b[1;34m(style)\u001b[0m\n\u001b[0;32m    137\u001b[0m         style \u001b[38;5;241m=\u001b[39m _rc_params_in_file(style)\n\u001b[0;32m    138\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[1;32m--> 139\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m(\n\u001b[0;32m    140\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mstyle\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[38;5;124m is not a valid package style, path of style \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    141\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfile, URL of style file, or library style name (library \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    142\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstyles are listed in `style.available`)\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merr\u001b[39;00m\n\u001b[0;32m    143\u001b[0m filtered \u001b[38;5;241m=\u001b[39m {}\n\u001b[0;32m    144\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m style:  \u001b[38;5;66;03m# don't trigger RcParams.__getitem__('backend')\u001b[39;00m\n",
      "\u001b[1;31mOSError\u001b[0m: 'seaborn-darkgrid' is not a valid package style, path of style file, URL of style file, or library style name (library styles are listed in `style.available`)"
     ]
    }
   ],
   "source": [
    "# 2. Setup and Libraries\n",
    "\n",
    "## Import Necessary Libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy.stats as stats\n",
    "import statsmodels.api as sm\n",
    "import statsmodels.formula.api as smf\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso, ElasticNet, LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler, PolynomialFeatures\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cluster import KMeans, AgglomerativeClustering, DBSCAN\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, KFold\n",
    "from sklearn.metrics import mean_squared_error, r2_score, silhouette_score, davies_bouldin_score\n",
    "from statsmodels.tsa.seasonal import seasonal_decompose\n",
    "from statsmodels.tsa.stattools import acf, pacf\n",
    "from statsmodels.tsa.arima.model import ARIMA\n",
    "\n",
    "# Configure visualizations\n",
    "%matplotlib inline\n",
    "plt.style.use('seaborn-darkgrid')\n",
    "\n",
    "## Setup Data\n",
    "# Load datasets or generate synthetic data\n",
    "# For demonstration, we'll use some well-known datasets from seaborn\n",
    "iris = sns.load_dataset('iris')\n",
    "tips = sns.load_dataset('tips')\n",
    "flights = sns.load_dataset('flights')\n",
    "\n",
    "# Display the first few rows of each dataset\n",
    "print(\"Iris Dataset:\")\n",
    "print(iris.head(), \"\\n\")\n",
    "print(\"Tips Dataset:\")\n",
    "print(tips.head(), \"\\n\")\n",
    "print(\"Flights Dataset:\")\n",
    "print(flights.head(), \"\\n\")\n",
    "\n",
    "# Additional setup for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "# Generate synthetic data for regression analysis\n",
    "X = 2 * np.random.rand(100, 1)\n",
    "y = 4 + 3 * X + np.random.randn(100, 1)\n",
    "\n",
    "# Create a DataFrame for convenience\n",
    "data = pd.DataFrame({'X': X.flatten(), 'y': y.flatten()})\n",
    "print(\"Synthetic Regression Data:\")\n",
    "print(data.head(), \"\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Descriptive Statistics\n",
    "\n",
    "**Definition**: Descriptive statistics summarize and describe the features of a dataset. <br>\n",
    "**Purpose**: They provide a simple overview of the sample and measures. <br>\n",
    "**Objectives**: In this section, we will explore these topics:\n",
    "\n",
    "- summary statistics\n",
    "- data visualization\n",
    "- correlation analysis\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Summary Statistics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary Statistics\n",
    "\n",
    "## Iris Dataset\n",
    "print(\"Summary Statistics for Iris Dataset:\")\n",
    "print(iris.describe(), \"\\n\")\n",
    "\n",
    "## Tips Dataset\n",
    "print(\"Summary Statistics for Tips Dataset:\")\n",
    "print(tips.describe(), \"\\n\")\n",
    "\n",
    "## Flights Dataset\n",
    "print(\"Summary Statistics for Flights Dataset:\")\n",
    "print(flights.describe(), \"\\n\")\n",
    "\n",
    "## Synthetic Regression Data\n",
    "print(\"Summary Statistics for Synthetic Regression Data:\")\n",
    "print(data.describe(), \"\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Data Visualization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Visualization\n",
    "\n",
    "## Histograms\n",
    "# Iris Dataset\n",
    "iris.hist(bins=20, figsize=(10, 8))\n",
    "plt.suptitle(\"Histograms of Iris Dataset Features\")\n",
    "plt.show()\n",
    "\n",
    "# Tips Dataset\n",
    "tips.hist(bins=20, figsize=(10, 8))\n",
    "plt.suptitle(\"Histograms of Tips Dataset Features\")\n",
    "plt.show()\n",
    "\n",
    "# Flights Dataset\n",
    "flights[\"passengers\"].hist(bins=20)\n",
    "plt.title(\"Histogram of Flights Dataset (Passengers)\")\n",
    "plt.xlabel(\"Passengers\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.show()\n",
    "\n",
    "# Synthetic Regression Data\n",
    "data.hist(bins=20, figsize=(10, 5))\n",
    "plt.suptitle(\"Histogram of Synthetic Regression Data\")\n",
    "plt.show()\n",
    "\n",
    "## Box Plots\n",
    "# Iris Dataset\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.boxplot(data=iris)\n",
    "plt.title(\"Box Plot of Iris Dataset Features\")\n",
    "plt.show()\n",
    "\n",
    "# Tips Dataset\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.boxplot(data=tips)\n",
    "plt.title(\"Box Plot of Tips Dataset Features\")\n",
    "plt.show()\n",
    "\n",
    "# Flights Dataset\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.boxplot(y=\"passengers\", data=flights)\n",
    "plt.title(\"Box Plot of Flights Dataset (Passengers)\")\n",
    "plt.show()\n",
    "\n",
    "# Synthetic Regression Data\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.boxplot(data=data)\n",
    "plt.title(\"Box Plot of Synthetic Regression Data\")\n",
    "plt.show()\n",
    "\n",
    "## Scatter Plots\n",
    "# Iris Dataset (Petal Length vs Petal Width)\n",
    "sns.scatterplot(x=\"petal_length\", y=\"petal_width\", data=iris, hue=\"species\")\n",
    "plt.title(\"Scatter Plot of Petal Length vs Petal Width (Iris Dataset)\")\n",
    "plt.show()\n",
    "\n",
    "# Tips Dataset (Total Bill vs Tip)\n",
    "sns.scatterplot(x=\"total_bill\", y=\"tip\", data=tips, hue=\"day\")\n",
    "plt.title(\"Scatter Plot of Total Bill vs Tip (Tips Dataset)\")\n",
    "plt.show()\n",
    "\n",
    "# Flights Dataset (Year vs Passengers)\n",
    "sns.lineplot(x=\"year\", y=\"passengers\", data=flights)\n",
    "plt.title(\"Line Plot of Year vs Passengers (Flights Dataset)\")\n",
    "plt.show()\n",
    "\n",
    "# Synthetic Regression Data (X vs y)\n",
    "sns.scatterplot(x=\"X\", y=\"y\", data=data)\n",
    "plt.title(\"Scatter Plot of X vs y (Synthetic Regression Data)\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Correlation Analysis\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation Analysis\n",
    "\n",
    "## Iris Dataset\n",
    "print(\"Correlation Matrix for Iris Dataset:\")\n",
    "print(iris.corr(), \"\\n\")\n",
    "\n",
    "sns.heatmap(iris.corr(), annot=True, cmap=\"coolwarm\")\n",
    "plt.title(\"Correlation Matrix Heatmap (Iris Dataset)\")\n",
    "plt.show()\n",
    "\n",
    "## Tips Dataset\n",
    "print(\"Correlation Matrix for Tips Dataset:\")\n",
    "print(tips.corr(), \"\\n\")\n",
    "\n",
    "sns.heatmap(tips.corr(), annot=True, cmap=\"coolwarm\")\n",
    "plt.title(\"Correlation Matrix Heatmap (Tips Dataset)\")\n",
    "plt.show()\n",
    "\n",
    "## Flights Dataset\n",
    "# For flights data, we need to pivot the dataset to calculate correlations for monthly data\n",
    "flights_pivot = flights.pivot(\"year\", \"month\", \"passengers\")\n",
    "print(\"Correlation Matrix for Flights Dataset:\")\n",
    "print(flights_pivot.corr(), \"\\n\")\n",
    "\n",
    "sns.heatmap(flights_pivot.corr(), annot=True, cmap=\"coolwarm\")\n",
    "plt.title(\"Correlation Matrix Heatmap (Flights Dataset)\")\n",
    "plt.show()\n",
    "\n",
    "## Synthetic Regression Data\n",
    "print(\"Correlation Matrix for Synthetic Regression Data:\")\n",
    "print(data.corr(), \"\\n\")\n",
    "\n",
    "sns.heatmap(data.corr(), annot=True, cmap=\"coolwarm\")\n",
    "plt.title(\"Correlation Matrix Heatmap (Synthetic Regression Data)\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Review: Section 3\n",
    "\n",
    "In this section, we performed:\n",
    "\n",
    "1. **Summary Statistics**: Provide a summary of each dataset's central tendency, dispersion, and shape.\n",
    "2. **Data Visualization**: Used histograms, box plots, scatter plots, and line plots to visualize the distribution and relationships within the data.\n",
    "3. **Correlation Analysis**: Calculate and visualize the correlation matrices to understand relationships between variables.\n",
    "\n",
    "This provides a thorough understanding of the datasets before moving on to more advanced statistical methods.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Inferential Statistics\n",
    "\n",
    "This code performs various inferential statistical analyses including hypothesis testing and calculating confidence intervals.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hypothesis Testing\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### One-sample t-test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Null hypothesis: The mean of the sample is equal to a specified value\n",
    "sample_data = data[\"y\"]\n",
    "t_stat, p_val = stats.ttest_1samp(sample_data, popmean=5)\n",
    "print(f\"One-sample t-test: t-statistic = {t_stat}, p-value = {p_val}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Independent t-test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Null hypothesis: The means of two independent samples are equal\n",
    "group1 = data[data[\"X\"] < 1][\"y\"]\n",
    "group2 = data[data[\"X\"] >= 1][\"y\"]\n",
    "t_stat, p_val = stats.ttest_ind(group1, group2)\n",
    "print(f\"Independent t-test: t-statistic = {t_stat}, p-value = {p_val}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Paired t-test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Null hypothesis: The means of two related samples are equal\n",
    "before = data[\"y\"]\n",
    "after = data[\"y\"] + np.random.randn(len(data))\n",
    "t_stat, p_val = stats.ttest_rel(before, after)\n",
    "print(f\"Paired t-test: t-statistic = {t_stat}, p-value = {p_val}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### One-way ANOVA\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Null hypothesis: The means of three or more groups are equal\n",
    "groups = [\n",
    "    data[\"y\"][data[\"X\"] < 0.5],\n",
    "    data[\"y\"][(data[\"X\"] >= 0.5) & (data[\"X\"] < 1.5)],\n",
    "    data[\"y\"][data[\"X\"] >= 1.5],\n",
    "]\n",
    "f_stat, p_val = stats.f_oneway(*groups)\n",
    "print(f\"One-way ANOVA: F-statistic = {f_stat}, p-value = {p_val}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Two-way ANOVA\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Null hypothesis: The means of groups defined by two factors are equal\n",
    "# Generate synthetic data for two-way ANOVA\n",
    "np.random.seed(42)\n",
    "factor_a = np.random.choice([\"A1\", \"A2\"], 100)\n",
    "factor_b = np.random.choice([\"B1\", \"B2\"], 100)\n",
    "response = 3 + 2 * (factor_a == \"A1\") + 1.5 * (factor_b == \"B1\") + np.random.randn(100)\n",
    "df = pd.DataFrame({\"factor_a\": factor_a, \"factor_b\": factor_b, \"response\": response})\n",
    "model = smf.ols(\"response ~ C(factor_a) * C(factor_b)\", data=df).fit()\n",
    "anova_table = sm.stats.anova_lm(model, typ=2)\n",
    "print(f\"Two-way ANOVA:\\n{anova_table}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Chi-square test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Null hypothesis: There is no association between two categorical variables\n",
    "contingency_table = pd.crosstab(df[\"factor_a\"], df[\"factor_b\"])\n",
    "chi2, p_val, dof, ex = stats.chi2_contingency(contingency_table)\n",
    "print(f\"Chi-square test: chi2 = {chi2}, p-value = {p_val}, dof = {dof}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Confidence Intervals\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Confidence interval for a mean\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_ci = stats.t.interval(\n",
    "    alpha=0.95,\n",
    "    df=len(sample_data) - 1,\n",
    "    loc=np.mean(sample_data),\n",
    "    scale=stats.sem(sample_data),\n",
    ")\n",
    "print(f\"95% confidence interval for the mean: {mean_ci}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Confidence interval for a proportion\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "proportion = 0.6\n",
    "n = 100\n",
    "ci_low, ci_upp = stats.binom.interval(alpha=0.95, n=n, p=proportion, loc=0)\n",
    "print(f\"95% confidence interval for a proportion: ({ci_low/n}, {ci_upp/n})\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Regression Analysis\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple Linear Regression\n",
    "X_simple = data[[\"X\"]]\n",
    "y_simple = data[\"y\"]\n",
    "model_simple = LinearRegression()\n",
    "model_simple.fit(X_simple, y_simple)\n",
    "y_pred_simple = model_simple.predict(X_simple)\n",
    "\n",
    "print(f\"Simple Linear Regression Coefficients: {model_simple.coef_}\")\n",
    "print(f\"Intercept: {model_simple.intercept_}\")\n",
    "print(f\"R-squared: {model_simple.score(X_simple, y_simple)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting the results\n",
    "plt.scatter(X_simple, y_simple, label=\"Data\")\n",
    "plt.plot(X_simple, y_pred_simple, color=\"red\", label=\"Fitted Line\")\n",
    "plt.title(\"Simple Linear Regression\")\n",
    "plt.xlabel(\"X\")\n",
    "plt.ylabel(\"y\")\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multiple Linear Regression\n",
    "X_multiple = pd.DataFrame({\"X1\": X.flatten(), \"X2\": X.flatten() ** 2})\n",
    "model_multiple = LinearRegression()\n",
    "model_multiple.fit(X_multiple, y_simple)\n",
    "y_pred_multiple = model_multiple.predict(X_multiple)\n",
    "\n",
    "print(f\"Multiple Linear Regression Coefficients: {model_multiple.coef_}\")\n",
    "print(f\"Intercept: {model_multiple.intercept_}\")\n",
    "print(f\"R-squared: {model_multiple.score(X_multiple, y_simple)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting the results\n",
    "plt.scatter(X_simple, y_simple, label=\"Data\")\n",
    "plt.plot(X_simple, y_pred_multiple, color=\"red\", label=\"Fitted Curve\")\n",
    "plt.title(\"Multiple Linear Regression\")\n",
    "plt.xlabel(\"X\")\n",
    "plt.ylabel(\"y\")\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Polynomial Regression\n",
    "poly_features = PolynomialFeatures(degree=2)\n",
    "X_poly = poly_features.fit_transform(X_simple)\n",
    "model_poly = LinearRegression()\n",
    "model_poly.fit(X_poly, y_simple)\n",
    "y_pred_poly = model_poly.predict(X_poly)\n",
    "\n",
    "print(f\"Polynomial Regression Coefficients: {model_poly.coef_}\")\n",
    "print(f\"Intercept: {model_poly.intercept_}\")\n",
    "print(f\"R-squared: {model_poly.score(X_poly, y_simple)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting the results\n",
    "plt.scatter(X_simple, y_simple, label=\"Data\")\n",
    "plt.plot(X_simple, y_pred_poly, color=\"red\", label=\"Fitted Polynomial Curve\")\n",
    "plt.title(\"Polynomial Regression\")\n",
    "plt.xlabel(\"X\")\n",
    "plt.ylabel(\"y\")\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Evaluation\n",
    "mse = mean_squared_error(y_simple, y_pred_simple)\n",
    "rmse = np.sqrt(mse)\n",
    "r2 = r2_score(y_simple, y_pred_simple)\n",
    "\n",
    "print(f\"Mean Squared Error (MSE): {mse}\")\n",
    "print(f\"Root Mean Squared Error (RMSE): {rmse}\")\n",
    "print(f\"R-squared: {r2}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Additional evaluation for multiple and polynomial regression\n",
    "mse_multiple = mean_squared_error(y_simple, y_pred_multiple)\n",
    "rmse_multiple = np.sqrt(mse_multiple)\n",
    "r2_multiple = r2_score(y_simple, y_pred_multiple)\n",
    "\n",
    "print(\n",
    "    f\"Multiple Regression - MSE: {mse_multiple}, RMSE: {rmse_multiple}, R-squared: {r2_multiple}\"\n",
    ")\n",
    "\n",
    "mse_poly = mean_squared_error(y_simple, y_pred_poly)\n",
    "rmse_poly = np.sqrt(mse_poly)\n",
    "r2_poly = r2_score(y_simple, y_pred_poly)\n",
    "\n",
    "print(\n",
    "    f\"Polynomial Regression - MSE: {mse_poly}, RMSE: {rmse_poly}, R-squared: {r2_poly}\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Regression Analyses Performed\n",
    "\n",
    "- **simple linear regression**\n",
    "- **multiple linear regression**\n",
    "- **polynomial regression**\n",
    "\n",
    "#### Model Evaluation Metrics\n",
    "\n",
    "- `Mean Squared Error (MSE)`\n",
    "- `Root Mean Squared Error (RMSE)`\n",
    "- `R-squared`.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Advanced Regression Techniques\n",
    "\n",
    "### 6.1 Ridge Regression\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ridge Regression\n",
    "from sklearn.linear_model import Ridge\n",
    "\n",
    "# Prepare the data\n",
    "X_ridge = data[[\"X\"]]\n",
    "y_ridge = data[\"y\"]\n",
    "\n",
    "# Create and fit the model\n",
    "ridge_model = Ridge(alpha=1.0)\n",
    "ridge_model.fit(X_ridge, y_ridge)\n",
    "\n",
    "# Predictions\n",
    "y_pred_ridge = ridge_model.predict(X_ridge)\n",
    "\n",
    "# Model evaluation\n",
    "ridge_mse = mean_squared_error(y_ridge, y_pred_ridge)\n",
    "ridge_rmse = np.sqrt(ridge_mse)\n",
    "ridge_r2 = r2_score(y_ridge, y_pred_ridge)\n",
    "\n",
    "print(f\"Ridge Regression Coefficients: {ridge_model.coef_}\")\n",
    "print(f\"Intercept: {ridge_model.intercept_}\")\n",
    "print(f\"Ridge Regression - MSE: {ridge_mse}, RMSE: {ridge_rmse}, R-squared: {ridge_r2}\")\n",
    "\n",
    "# Plotting the results\n",
    "plt.scatter(X_ridge, y_ridge, label=\"Data\")\n",
    "plt.plot(X_ridge, y_pred_ridge, color=\"red\", label=\"Ridge Regression Line\")\n",
    "plt.title(\"Ridge Regression\")\n",
    "plt.xlabel(\"X\")\n",
    "plt.ylabel(\"y\")\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2 Lasso Regression\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lasso Regression\n",
    "from sklearn.linear_model import Lasso\n",
    "\n",
    "# Prepare the data\n",
    "X_lasso = data[[\"X\"]]\n",
    "y_lasso = data[\"y\"]\n",
    "\n",
    "# Create and fit the model\n",
    "lasso_model = Lasso(alpha=0.1)\n",
    "lasso_model.fit(X_lasso, y_lasso)\n",
    "\n",
    "# Predictions\n",
    "y_pred_lasso = lasso_model.predict(X_lasso)\n",
    "\n",
    "# Model evaluation\n",
    "lasso_mse = mean_squared_error(y_lasso, y_pred_lasso)\n",
    "lasso_rmse = np.sqrt(lasso_mse)\n",
    "lasso_r2 = r2_score(y_lasso, y_pred_lasso)\n",
    "\n",
    "print(f\"Lasso Regression Coefficients: {lasso_model.coef_}\")\n",
    "print(f\"Intercept: {lasso_model.intercept_}\")\n",
    "print(f\"Lasso Regression - MSE: {lasso_mse}, RMSE: {lasso_rmse}, R-squared: {lasso_r2}\")\n",
    "\n",
    "# Plotting the results\n",
    "plt.scatter(X_lasso, y_lasso, label=\"Data\")\n",
    "plt.plot(X_lasso, y_pred_lasso, color=\"red\", label=\"Lasso Regression Line\")\n",
    "plt.title(\"Lasso Regression\")\n",
    "plt.xlabel(\"X\")\n",
    "plt.ylabel(\"y\")\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code performs Ridge and Lasso regression, fits the models to the data, makes predictions, evaluates the models using MSE, RMSE, and R-squared, and visualizes the fitted regression lines.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.3 Elastic Net Regression\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Elastic Net Regression\n",
    "from sklearn.linear_model import ElasticNet\n",
    "\n",
    "# Prepare the data\n",
    "X_elastic = data[[\"X\"]]\n",
    "y_elastic = data[\"y\"]\n",
    "\n",
    "# Create and fit the model\n",
    "elastic_model = ElasticNet(alpha=0.1, l1_ratio=0.5)\n",
    "elastic_model.fit(X_elastic, y_elastic)\n",
    "\n",
    "# Predictions\n",
    "y_pred_elastic = elastic_model.predict(X_elastic)\n",
    "\n",
    "# Model evaluation\n",
    "elastic_mse = mean_squared_error(y_elastic, y_pred_elastic)\n",
    "elastic_rmse = np.sqrt(elastic_mse)\n",
    "elastic_r2 = r2_score(y_elastic, y_pred_elastic)\n",
    "\n",
    "print(f\"Elastic Net Regression Coefficients: {elastic_model.coef_}\")\n",
    "print(f\"Intercept: {elastic_model.intercept_}\")\n",
    "print(\n",
    "    f\"Elastic Net Regression - MSE: {elastic_mse}, RMSE: {elastic_rmse}, R-squared: {elastic_r2}\"\n",
    ")\n",
    "\n",
    "# Plotting the results\n",
    "plt.scatter(X_elastic, y_elastic, label=\"Data\")\n",
    "plt.plot(X_elastic, y_pred_elastic, color=\"red\", label=\"Elastic Net Regression Line\")\n",
    "plt.title(\"Elastic Net Regression\")\n",
    "plt.xlabel(\"X\")\n",
    "plt.ylabel(\"y\")\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.4 Logistic Regression\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logistic Regression\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Generate synthetic binary data for logistic regression\n",
    "np.random.seed(42)\n",
    "X_logistic = np.random.rand(100, 1) * 10\n",
    "y_logistic = (X_logistic.flatten() > 5).astype(int)\n",
    "\n",
    "# Create and fit the model\n",
    "logistic_model = LogisticRegression()\n",
    "logistic_model.fit(X_logistic, y_logistic)\n",
    "\n",
    "# Predictions\n",
    "y_pred_logistic = logistic_model.predict(X_logistic)\n",
    "y_pred_prob = logistic_model.predict_proba(X_logistic)[:, 1]\n",
    "\n",
    "# Model evaluation\n",
    "logistic_accuracy = logistic_model.score(X_logistic, y_logistic)\n",
    "print(f\"Logistic Regression Accuracy: {logistic_accuracy}\")\n",
    "\n",
    "# Plotting the results\n",
    "plt.scatter(X_logistic, y_logistic, label=\"Data\", alpha=0.5)\n",
    "plt.plot(X_logistic, y_pred_prob, color=\"red\", label=\"Logistic Regression Probability\")\n",
    "plt.title(\"Logistic Regression\")\n",
    "plt.xlabel(\"X\")\n",
    "plt.ylabel(\"Probability\")\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code performs Elastic Net and Logistic regression, fits the models to the data, makes predictions, evaluates the models using metrics such as `MSE`, `RMSE`, `R-squared for Elastic Net`, and `accuracy` for Logistic regression, and visualizes the fitted regression lines.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Time Series Analysis\n",
    "\n",
    "### 7.1 Introduction to Time Series Data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Introduction to Time Series Data\n",
    "\n",
    "# Display the first few rows of the Flights dataset to understand its structure\n",
    "print(\"Flights Dataset:\")\n",
    "print(flights.head())\n",
    "\n",
    "# Convert 'year' and 'month' to a datetime format for proper time series handling\n",
    "flights[\"date\"] = pd.to_datetime(flights.assign(day=1).loc[:, [\"year\", \"month\", \"day\"]])\n",
    "flights.set_index(\"date\", inplace=True)\n",
    "flights.sort_index(inplace=True)\n",
    "\n",
    "# Plot the time series data\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(flights.index, flights[\"passengers\"], label=\"Number of Passengers\")\n",
    "plt.title(\"Number of Passengers Over Time\")\n",
    "plt.xlabel(\"Date\")\n",
    "plt.ylabel(\"Number of Passengers\")\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.2 Decomposition of Time Series\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decomposition of Time Series\n",
    "\n",
    "# Perform seasonal decomposition\n",
    "decomposition = seasonal_decompose(flights[\"passengers\"], model=\"multiplicative\")\n",
    "\n",
    "# Extract and plot the decomposed components\n",
    "trend = decomposition.trend\n",
    "seasonal = decomposition.seasonal\n",
    "residual = decomposition.resid\n",
    "\n",
    "plt.figure(figsize=(12, 10))\n",
    "\n",
    "plt.subplot(411)\n",
    "plt.plot(flights[\"passengers\"], label=\"Original\")\n",
    "plt.legend(loc=\"upper left\")\n",
    "\n",
    "plt.subplot(412)\n",
    "plt.plot(trend, label=\"Trend\")\n",
    "plt.legend(loc=\"upper left\")\n",
    "\n",
    "plt.subplot(413)\n",
    "plt.plot(seasonal, label=\"Seasonality\")\n",
    "plt.legend(loc=\"upper left\")\n",
    "\n",
    "plt.subplot(414)\n",
    "plt.plot(residual, label=\"Residuals\")\n",
    "plt.legend(loc=\"upper left\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.3 Autocorrelation and Partial Autocorrelation Functions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Autocorrelation and Partial Autocorrelation Functions\n",
    "\n",
    "# Autocorrelation function (ACF)\n",
    "plt.figure(figsize=(12, 6))\n",
    "acf_values = acf(flights[\"passengers\"], nlags=40)\n",
    "plt.stem(range(len(acf_values)), acf_values, use_line_collection=True)\n",
    "plt.title(\"Autocorrelation Function (ACF)\")\n",
    "plt.xlabel(\"Lags\")\n",
    "plt.ylabel(\"ACF\")\n",
    "plt.show()\n",
    "\n",
    "# Partial Autocorrelation function (PACF)\n",
    "plt.figure(figsize=(12, 6))\n",
    "pacf_values = pacf(flights[\"passengers\"], nlags=40)\n",
    "plt.stem(range(len(pacf_values)), pacf_values, use_line_collection=True)\n",
    "plt.title(\"Partial Autocorrelation Function (PACF)\")\n",
    "plt.xlabel(\"Lags\")\n",
    "plt.ylabel(\"PACF\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.4 ARIMA and SARIMA Models\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ARIMA Model\n",
    "from statsmodels.tsa.arima.model import ARIMA\n",
    "\n",
    "# Fit ARIMA model\n",
    "arima_model = ARIMA(flights[\"passengers\"], order=(2, 1, 2))\n",
    "arima_result = arima_model.fit()\n",
    "\n",
    "# Forecast\n",
    "arima_forecast = arima_result.get_forecast(steps=12)\n",
    "arima_pred_ci = arima_forecast.conf_int()\n",
    "\n",
    "# Plotting the results\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(flights.index, flights[\"passengers\"], label=\"Observed\")\n",
    "plt.plot(\n",
    "    arima_forecast.predicted_mean.index, arima_forecast.predicted_mean, label=\"Forecast\"\n",
    ")\n",
    "plt.fill_between(\n",
    "    arima_pred_ci.index,\n",
    "    arima_pred_ci.iloc[:, 0],\n",
    "    arima_pred_ci.iloc[:, 1],\n",
    "    color=\"k\",\n",
    "    alpha=0.1,\n",
    ")\n",
    "plt.title(\"ARIMA Model Forecast\")\n",
    "plt.xlabel(\"Date\")\n",
    "plt.ylabel(\"Number of Passengers\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# SARIMA Model\n",
    "from statsmodels.tsa.statespace.sarimax import SARIMAX\n",
    "\n",
    "# Fit SARIMA model\n",
    "sarima_model = SARIMAX(\n",
    "    flights[\"passengers\"], order=(1, 1, 1), seasonal_order=(1, 1, 1, 12)\n",
    ")\n",
    "sarima_result = sarima_model.fit()\n",
    "\n",
    "# Forecast\n",
    "sarima_forecast = sarima_result.get_forecast(steps=12)\n",
    "sarima_pred_ci = sarima_forecast.conf_int()\n",
    "\n",
    "# Plotting the results\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(flights.index, flights[\"passengers\"], label=\"Observed\")\n",
    "plt.plot(\n",
    "    sarima_forecast.predicted_mean.index,\n",
    "    sarima_forecast.predicted_mean,\n",
    "    label=\"Forecast\",\n",
    ")\n",
    "plt.fill_between(\n",
    "    sarima_pred_ci.index,\n",
    "    sarima_pred_ci.iloc[:, 0],\n",
    "    sarima_pred_ci.iloc[:, 1],\n",
    "    color=\"k\",\n",
    "    alpha=0.1,\n",
    ")\n",
    "plt.title(\"SARIMA Model Forecast\")\n",
    "plt.xlabel(\"Date\")\n",
    "plt.ylabel(\"Number of Passengers\")\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Review\n",
    "\n",
    "1. **Introduction to Time Series Data**: Introduces the time series data by displaying and plotting the Flights dataset with the number of passengers over time.\n",
    "2. **Decomposition of Time Series**: Decomposes the time series data into its trend, seasonal, and residual components using seasonal decomposition and visualizes these components.\n",
    "3. **Autocorrelation and Partial Autocorrelation Functions**:\n",
    "   - Computes and plots the Autocorrelation Function (ACF) and Partial Autocorrelation Function (PACF) for the time series data.\n",
    "4. **ARIMA and SARIMA Models**:\n",
    "   - Fits an ARIMA model to the time series data, forecasts future values, and plots the forecast with confidence intervals.\n",
    "   - Fits a SARIMA model to the time series data, forecasts future values, and plots the forecast with confidence intervals.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Principal Component Analysis (PCA)\n",
    "\n",
    "Principal Component Analysis (PCA) is a dimensionality reduction technique that transforms data into a new coordinate system where the greatest variances lie on the first few principal components. This section covers the steps to perform PCA and interpret its results.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.1 Introduction to PCA\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the Iris dataset for PCA\n",
    "print(\"Iris Dataset:\")\n",
    "print(iris.head())\n",
    "\n",
    "# Standardize the data\n",
    "features = [\"sepal_length\", \"sepal_width\", \"petal_length\", \"petal_width\"]\n",
    "x = iris.loc[:, features].values\n",
    "y = iris.loc[:, [\"species\"]].values\n",
    "x = StandardScaler().fit_transform(x)\n",
    "\n",
    "# Perform PCA\n",
    "pca = PCA(n_components=2)\n",
    "principal_components = pca.fit_transform(x)\n",
    "principal_df = pd.DataFrame(\n",
    "    data=principal_components,\n",
    "    columns=[\"principal_component_1\", \"principal_component_2\"],\n",
    ")\n",
    "\n",
    "# Concatenate the target variable\n",
    "final_df = pd.concat([principal_df, iris[[\"species\"]]], axis=1)\n",
    "\n",
    "# Display the explained variance ratio\n",
    "print(\"Explained Variance Ratio:\")\n",
    "print(pca.explained_variance_ratio_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.2 Visualizing Principal Components\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizing Principal Components\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.scatterplot(\n",
    "    data=final_df, x=\"principal_component_1\", y=\"principal_component_2\", hue=\"species\"\n",
    ")\n",
    "plt.title(\"2D PCA of Iris Dataset\")\n",
    "plt.xlabel(\"Principal Component 1\")\n",
    "plt.ylabel(\"Principal Component 2\")\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.3 Explained Variance\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explained Variance\n",
    "\n",
    "explained_variance = np.cumsum(pca.explained_variance_ratio_)\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(range(1, len(explained_variance) + 1), explained_variance, marker='o', linestyle='--')\n",
    "plt.title('Explained Variance by Principal Components')\n",
    "plt.xlabel('Number of Principal Components')\n",
    "plt.ylabel('Cumulative Explained Variance')\n",
    "plt.show()\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.4 Interpretation of Principal Components\n",
    "\n",
    "The first principal component captures the most variance in the dataset, and each subsequent component captures the remaining variance under the constraint that it is orthogonal to the previous components. Interpretation of principal components involves understanding which original features contribute most to each component, which can be done by examining the component loadings.\n",
    "\n",
    "### Component Loadings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loadings = pca.components_.T\n",
    "components_df = pd.DataFrame(loadings, columns=[\"PC1\", \"PC2\"], index=features)\n",
    "print(\"Principal Components Loadings:\")\n",
    "print(components_df)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.heatmap(components_df, annot=True, cmap=\"coolwarm\")\n",
    "plt.title(\"Heatmap of Principal Components Loadings\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Notes on Expanding PCA Topics\n",
    "\n",
    "1. **Increasing the Number of Components**:\n",
    "\n",
    "   - Expand the analysis by considering more principal components to capture more variance.\n",
    "   - Plot 3D PCA if three principal components are considered.\n",
    "\n",
    "2. **Using PCA for Classification**:\n",
    "\n",
    "   - Apply PCA as a preprocessing step before classification algorithms like K-Nearest Neighbors (KNN), Support Vector Machines (SVM), or logistic regression.\n",
    "   - Compare the performance of classifiers with and without PCA.\n",
    "\n",
    "3. **Interpreting Loadings in Depth**:\n",
    "\n",
    "   - Analyze the loadings to understand the influence of each feature on the principal components.\n",
    "   - Visualize loadings as bar plots for better interpretation.\n",
    "\n",
    "4. **Comparison with Other Dimensionality Reduction Techniques**:\n",
    "\n",
    "   - Compare PCA with other dimensionality reduction techniques such as Linear Discriminant Analysis (LDA), t-Distributed Stochastic Neighbor Embedding (t-SNE), and Uniform Manifold Approximation and Projection (UMAP).\n",
    "\n",
    "5. **Handling Non-linear Data**:\n",
    "\n",
    "   - Discuss limitations of PCA in handling non-linear relationships.\n",
    "   - Introduce Kernel PCA as an extension to capture non-linear patterns.\n",
    "\n",
    "6. **Applying PCA to Larger Datasets**:\n",
    "\n",
    "   - Use PCA on more complex and larger datasets (e.g., image datasets, genomic data).\n",
    "   - Explore optimization techniques for faster computation, such as Incremental PCA for large datasets.\n",
    "\n",
    "7. **Using PCA for Noise Reduction**:\n",
    "   - Apply PCA to denoise datasets by retaining only the components with significant variance.\n",
    "   - Compare the original and denoised datasets to evaluate the effectiveness.\n",
    "\n",
    "By expanding on these topics, you can gain a deeper understanding of PCA and its applications, limitations, and comparisons with other dimensionality reduction techniques.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Clustering\n",
    "\n",
    "### 9.1 K-means Clustering\n",
    "\n",
    "K-means clustering is an unsupervised machine learning algorithm that partitions the dataset into K clusters, where each data point belongs to the cluster with the nearest mean. This section covers the steps to perform K-means clustering and interpret its results.\n",
    "\n",
    "### 9.1.1 Prepare the Data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the Iris dataset for clustering\n",
    "print(\"Iris Dataset:\")\n",
    "print(iris.head())\n",
    "\n",
    "# Standardize the data\n",
    "features = [\"sepal_length\", \"sepal_width\", \"petal_length\", \"petal_width\"]\n",
    "x = iris.loc[:, features].values\n",
    "x = StandardScaler().fit_transform(x)\n",
    "\n",
    "# Convert to DataFrame for consistency\n",
    "df = pd.DataFrame(x, columns=features)\n",
    "print(\"Standardized Iris Dataset:\")\n",
    "print(df.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9.1.2 Apply K-means Clustering\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determine the optimal number of clusters using the elbow method\n",
    "sse = []\n",
    "k_range = range(1, 11)\n",
    "for k in k_range:\n",
    "    kmeans = KMeans(n_clusters=k, random_state=42)\n",
    "    kmeans.fit(df)\n",
    "    sse.append(kmeans.inertia_)\n",
    "\n",
    "# Plot the SSE for the elbow method\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(k_range, sse, marker=\"o\", linestyle=\"--\")\n",
    "plt.title(\"Elbow Method for Optimal Number of Clusters\")\n",
    "plt.xlabel(\"Number of Clusters\")\n",
    "plt.ylabel(\"Sum of Squared Errors (SSE)\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9.1.3 Fit the K-means Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the K-means Model\n",
    "\n",
    "# Choose the optimal number of clusters based on the elbow plot\n",
    "optimal_k = 3\n",
    "kmeans = KMeans(n_clusters=optimal_k, random_state=42)\n",
    "kmeans.fit(df)\n",
    "\n",
    "# Add the cluster labels to the original DataFrame\n",
    "iris[\"cluster\"] = kmeans.labels_\n",
    "print(\"Iris Dataset with Cluster Labels:\")\n",
    "print(iris.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9.1.4 Visualize the Clusters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the Clusters\n",
    "\n",
    "# Visualize the clusters in 2D space using the first two principal components\n",
    "pca = PCA(n_components=2)\n",
    "principal_components = pca.fit_transform(df)\n",
    "principal_df = pd.DataFrame(\n",
    "    data=principal_components,\n",
    "    columns=[\"principal_component_1\", \"principal_component_2\"],\n",
    ")\n",
    "principal_df[\"cluster\"] = iris[\"cluster\"]\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.scatterplot(\n",
    "    data=principal_df,\n",
    "    x=\"principal_component_1\",\n",
    "    y=\"principal_component_2\",\n",
    "    hue=\"cluster\",\n",
    "    palette=\"viridis\",\n",
    ")\n",
    "plt.title(\"K-means Clustering of Iris Dataset (2D PCA)\")\n",
    "plt.xlabel(\"Principal Component 1\")\n",
    "plt.ylabel(\"Principal Component 2\")\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9.1.5 Evaluate the Clustering Results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Silhouette Score\n",
    "silhouette_avg = silhouette_score(df, kmeans.labels_)\n",
    "print(f\"Silhouette Score: {silhouette_avg}\")\n",
    "\n",
    "# Davies-Bouldin Score\n",
    "davies_bouldin = davies_bouldin_score(df, kmeans.labels_)\n",
    "print(f\"Davies-Bouldin Score: {davies_bouldin}\")\n",
    "\n",
    "# Detailed cluster analysis\n",
    "print(\"Cluster Centers:\")\n",
    "print(kmeans.cluster_centers_)\n",
    "\n",
    "print(\"Cluster Counts:\")\n",
    "print(iris[\"cluster\"].value_counts())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Notes on Expanding Clustering Analysis\n",
    "\n",
    "1. **Cluster Initialization**:\n",
    "\n",
    "   - Explore different initialization methods for the cluster centers, such as the 'k-means++' initialization.\n",
    "   - Perform multiple runs with different initializations to ensure stability and robustness of the clusters.\n",
    "\n",
    "2. **Handling Different Distance Metrics**:\n",
    "\n",
    "   - Extend the analysis by using different distance metrics (e.g., Manhattan distance, cosine similarity) and comparing the results.\n",
    "\n",
    "3. **Cluster Validation Techniques**:\n",
    "\n",
    "   - Use additional validation techniques such as the Calinski-Harabasz Index and Dunn Index to evaluate the clustering results.\n",
    "\n",
    "4. **Visualizing Higher Dimensions**:\n",
    "\n",
    "   - Use t-SNE or UMAP for visualizing high-dimensional data in 2D or 3D space to better understand the cluster structures.\n",
    "\n",
    "5. **Compare with Other Clustering Algorithms**:\n",
    "\n",
    "   - Compare K-means clustering with other clustering algorithms like Hierarchical Clustering, DBSCAN, and Gaussian Mixture Models (GMM).\n",
    "   - Discuss the advantages and disadvantages of each method in different scenarios.\n",
    "\n",
    "6. **Cluster Analysis and Interpretation**:\n",
    "\n",
    "   - Perform a detailed analysis of each cluster by examining the mean and variance of each feature within clusters.\n",
    "   - Use box plots or violin plots to visualize the distribution of features within each cluster.\n",
    "\n",
    "7. **Clustering on Different Datasets**:\n",
    "   - Apply K-means clustering on different datasets (e.g., customer segmentation, image data) to explore its versatility and applicability.\n",
    "\n",
    "By expanding on these topics, you can gain a deeper understanding of K-means clustering, its applications, and limitations, and compare it with other clustering techniques.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9.2 Hierarchical Clustering\n",
    "\n",
    "Hierarchical clustering is an unsupervised machine learning algorithm that builds a hierarchy of clusters. It can be agglomerative (bottom-up approach) or divisive (top-down approach). This section covers the steps to perform hierarchical clustering and interpret its results.\n",
    "\n",
    "### 9.2.1 Prepare the Data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the Iris dataset for clustering\n",
    "print(\"Iris Dataset:\")\n",
    "print(iris.head())\n",
    "\n",
    "# Standardize the data\n",
    "features = [\"sepal_length\", \"sepal_width\", \"petal_length\", \"petal_width\"]\n",
    "x = iris.loc[:, features].values\n",
    "x = StandardScaler().fit_transform(x)\n",
    "\n",
    "# Convert to DataFrame for consistency\n",
    "df = pd.DataFrame(x, columns=features)\n",
    "print(\"Standardized Iris Dataset:\")\n",
    "print(df.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9.2.2 Perform Hierarchical Clustering\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.cluster.hierarchy import dendrogram, linkage\n",
    "from scipy.cluster.hierarchy import fcluster\n",
    "\n",
    "# Generate the linkage matrix\n",
    "Z = linkage(df, method=\"ward\")\n",
    "\n",
    "# Plot the dendrogram\n",
    "plt.figure(figsize=(12, 8))\n",
    "dendrogram(Z, labels=iris.index, leaf_rotation=90, leaf_font_size=10)\n",
    "plt.title(\"Hierarchical Clustering Dendrogram (Ward Linkage)\")\n",
    "plt.xlabel(\"Sample Index\")\n",
    "plt.ylabel(\"Distance\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9.2.3 Determine the Optimal Number of Clusters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cut the dendrogram at the optimal number of clusters\n",
    "optimal_num_clusters = 3\n",
    "clusters = fcluster(Z, optimal_num_clusters, criterion=\"maxclust\")\n",
    "\n",
    "# Add the cluster labels to the original DataFrame\n",
    "iris[\"cluster\"] = clusters\n",
    "print(\"Iris Dataset with Cluster Labels:\")\n",
    "print(iris.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9.2.4 Visualize the Clusters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the clusters in 2D space using the first two principal components\n",
    "pca = PCA(n_components=2)\n",
    "principal_components = pca.fit_transform(df)\n",
    "principal_df = pd.DataFrame(\n",
    "    data=principal_components,\n",
    "    columns=[\"principal_component_1\", \"principal_component_2\"],\n",
    ")\n",
    "principal_df[\"cluster\"] = iris[\"cluster\"]\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.scatterplot(\n",
    "    data=principal_df,\n",
    "    x=\"principal_component_1\",\n",
    "    y=\"principal_component_2\",\n",
    "    hue=\"cluster\",\n",
    "    palette=\"viridis\",\n",
    ")\n",
    "plt.title(\"Hierarchical Clustering of Iris Dataset (2D PCA)\")\n",
    "plt.xlabel(\"Principal Component 1\")\n",
    "plt.ylabel(\"Principal Component 2\")\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9.2.5 Evaluate the Clustering Results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Silhouette Score\n",
    "silhouette_avg = silhouette_score(df, iris[\"cluster\"])\n",
    "print(f\"Silhouette Score: {silhouette_avg}\")\n",
    "\n",
    "# Davies-Bouldin Score\n",
    "davies_bouldin = davies_bouldin_score(df, iris[\"cluster\"])\n",
    "print(f\"Davies-Bouldin Score: {davies_bouldin}\")\n",
    "\n",
    "# Detailed cluster analysis\n",
    "print(\"Cluster Counts:\")\n",
    "print(iris[\"cluster\"].value_counts())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Notes on Expanding Hierarchical Clustering Analysis\n",
    "\n",
    "1. **Linkage Methods**:\n",
    "\n",
    "   - Explore different linkage methods such as single, complete, average, and centroid linkage.\n",
    "   - Compare the results and dendrograms of different linkage methods.\n",
    "\n",
    "2. **Distance Metrics**:\n",
    "\n",
    "   - Use different distance metrics like Euclidean, Manhattan, and cosine distance.\n",
    "   - Analyze the impact of different distance metrics on clustering results.\n",
    "\n",
    "3. **Visualization of High-dimensional Data**:\n",
    "\n",
    "   - Use t-SNE or UMAP for visualizing high-dimensional data in 2D or 3D space to better understand the cluster structures.\n",
    "\n",
    "4. **Agglomerative vs. Divisive Clustering**:\n",
    "\n",
    "   - Compare agglomerative clustering with divisive clustering.\n",
    "   - Discuss scenarios where one might be preferred over the other.\n",
    "\n",
    "5. **Cluster Validation Techniques**:\n",
    "\n",
    "   - Use additional validation techniques such as the Calinski-Harabasz Index and Dunn Index to evaluate the clustering results.\n",
    "\n",
    "6. **Detailed Cluster Analysis**:\n",
    "\n",
    "   - Perform a detailed analysis of each cluster by examining the mean and variance of each feature within clusters.\n",
    "   - Use box plots or violin plots to visualize the distribution of features within each cluster.\n",
    "\n",
    "7. **Handling Large Datasets**:\n",
    "\n",
    "   - Explore efficient implementations and optimizations for hierarchical clustering on large datasets.\n",
    "   - Use methods like hierarchical clustering with truncated linkage for large datasets.\n",
    "\n",
    "8. **Comparison with Other Clustering Algorithms**:\n",
    "   - Compare hierarchical clustering with other clustering algorithms like K-means, DBSCAN, and Gaussian Mixture Models (GMM).\n",
    "   - Discuss the advantages and disadvantages of each method in different scenarios.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9.3 DBSCAN (Density-Based Spatial Clustering of Applications with Noise)\n",
    "\n",
    "DBSCAN is an unsupervised clustering algorithm that groups together points that are closely packed together, marking points that are far away from others as outliers. This section covers the steps to perform DBSCAN clustering and interpret its results.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9.3.1 Prepare the Data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the Iris dataset for clustering\n",
    "print(\"Iris Dataset:\")\n",
    "print(iris.head())\n",
    "\n",
    "# Standardize the data\n",
    "features = [\"sepal_length\", \"sepal_width\", \"petal_length\", \"petal_width\"]\n",
    "x = iris.loc[:, features].values\n",
    "x = StandardScaler().fit_transform(x)\n",
    "\n",
    "# Convert to DataFrame for consistency\n",
    "df = pd.DataFrame(x, columns=features)\n",
    "print(\"Standardized Iris Dataset:\")\n",
    "print(df.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9.3.2 Apply DBSCAN\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import DBSCAN\n",
    "\n",
    "# Perform DBSCAN clustering\n",
    "dbscan = DBSCAN(eps=0.5, min_samples=5)\n",
    "dbscan.fit(df)\n",
    "\n",
    "# Add the cluster labels to the original DataFrame\n",
    "iris[\"cluster\"] = dbscan.labels_\n",
    "print(\"Iris Dataset with Cluster Labels:\")\n",
    "print(iris.head())\n",
    "\n",
    "# Count the number of points in each cluster\n",
    "cluster_counts = iris[\"cluster\"].value_counts()\n",
    "print(\"Cluster Counts:\")\n",
    "print(cluster_counts)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9.3.3 Visualize the Clusters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the clusters in 2D space using the first two principal components\n",
    "pca = PCA(n_components=2)\n",
    "principal_components = pca.fit_transform(df)\n",
    "principal_df = pd.DataFrame(\n",
    "    data=principal_components,\n",
    "    columns=[\"principal_component_1\", \"principal_component_2\"],\n",
    ")\n",
    "principal_df[\"cluster\"] = iris[\"cluster\"]\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.scatterplot(\n",
    "    data=principal_df,\n",
    "    x=\"principal_component_1\",\n",
    "    y=\"principal_component_2\",\n",
    "    hue=\"cluster\",\n",
    "    palette=\"viridis\",\n",
    ")\n",
    "plt.title(\"DBSCAN Clustering of Iris Dataset (2D PCA)\")\n",
    "plt.xlabel(\"Principal Component 1\")\n",
    "plt.ylabel(\"Principal Component 2\")\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9.3.4 Evaluate the Clustering Results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the Clustering Results\n",
    "\n",
    "# Silhouette Score (excluding noise points)\n",
    "silhouette_avg = silhouette_score(\n",
    "    df[dbscan.labels_ != -1], dbscan.labels_[dbscan.labels_ != -1]\n",
    ")\n",
    "print(f\"Silhouette Score: {silhouette_avg}\")\n",
    "\n",
    "# Davies-Bouldin Score (excluding noise points)\n",
    "davies_bouldin = davies_bouldin_score(\n",
    "    df[dbscan.labels_ != -1], dbscan.labels_[dbscan.labels_ != -1]\n",
    ")\n",
    "print(f\"Davies-Bouldin Score: {davies_bouldin}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Notes on Expanding DBSCAN Analysis\n",
    "\n",
    "1. **Parameter Tuning**:\n",
    "\n",
    "   - Explore different values for `eps` (the maximum distance between two samples for one to be considered as in the neighborhood of the other) and `min_samples` (the number of samples in a neighborhood for a point to be considered as a core point).\n",
    "   - Use grid search or other optimization techniques to find the optimal parameters.\n",
    "\n",
    "2. **Handling High-dimensional Data**:\n",
    "\n",
    "   - Apply DBSCAN to high-dimensional datasets and use techniques like PCA or t-SNE for visualization.\n",
    "   - Discuss how the curse of dimensionality affects the performance of DBSCAN.\n",
    "\n",
    "3. **Comparison with Other Clustering Algorithms**:\n",
    "\n",
    "   - Compare DBSCAN with other clustering algorithms like K-means, Hierarchical Clustering, and Gaussian Mixture Models (GMM).\n",
    "   - Highlight the advantages and disadvantages of DBSCAN, such as its ability to find arbitrarily shaped clusters and to handle noise.\n",
    "\n",
    "4. **Visualizing High-dimensional Clusters**:\n",
    "\n",
    "   - Use advanced visualization techniques like t-SNE or UMAP to visualize high-dimensional data in 2D or 3D space.\n",
    "\n",
    "5. **Cluster Validation Techniques**:\n",
    "\n",
    "   - Use additional validation techniques such as the Calinski-Harabasz Index and Dunn Index to evaluate the clustering results.\n",
    "\n",
    "6. **Applying DBSCAN to Different Datasets**:\n",
    "\n",
    "   - Apply DBSCAN to different types of datasets, such as geographical data, image data, and time-series data.\n",
    "   - Discuss the challenges and considerations specific to each dataset type.\n",
    "\n",
    "7. **Noise Handling and Outlier Detection**:\n",
    "\n",
    "   - Analyze the points labeled as noise by DBSCAN and discuss their significance.\n",
    "   - Use DBSCAN for outlier detection and compare it with other outlier detection methods.\n",
    "\n",
    "8. **Scaling and Preprocessing**:\n",
    "   - Discuss the importance of scaling and preprocessing data before applying DBSCAN.\n",
    "   - Compare different scaling techniques (e.g., StandardScaler, MinMaxScaler) and their impact on the clustering results.\n",
    "\n",
    "<!-- By expanding on these topics, you can gain a deeper understanding of DBSCAN, its applications, and limitations, and compare it with other clustering techniques. -->\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9.4 Gaussian Mixture Models (GMM)\n",
    "\n",
    "Gaussian Mixture Models (GMM) are probabilistic models that assume the data is generated from a mixture of several Gaussian distributions with unknown parameters. This section covers the steps to perform GMM clustering and interpret its results.\n",
    "\n",
    "### 9.4.1 Prepare the Data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the Iris dataset for clustering\n",
    "print(\"Iris Dataset:\")\n",
    "print(iris.head())\n",
    "\n",
    "# Standardize the data\n",
    "features = [\"sepal_length\", \"sepal_width\", \"petal_length\", \"petal_width\"]\n",
    "x = iris.loc[:, features].values\n",
    "x = StandardScaler().fit_transform(x)\n",
    "\n",
    "# Convert to DataFrame for consistency\n",
    "df = pd.DataFrame(x, columns=features)\n",
    "print(\"Standardized Iris Dataset:\")\n",
    "print(df.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9.4.2 Apply GMM Clustering\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.mixture import GaussianMixture\n",
    "\n",
    "# Determine the optimal number of clusters using the Bayesian Information Criterion (BIC)\n",
    "bic = []\n",
    "k_range = range(1, 11)\n",
    "for k in k_range:\n",
    "    gmm = GaussianMixture(n_components=k, random_state=42)\n",
    "    gmm.fit(df)\n",
    "    bic.append(gmm.bic(df))\n",
    "\n",
    "# Plot the BIC for the elbow method\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(k_range, bic, marker=\"o\", linestyle=\"--\")\n",
    "plt.title(\"BIC for Optimal Number of Clusters\")\n",
    "plt.xlabel(\"Number of Clusters\")\n",
    "plt.ylabel(\"Bayesian Information Criterion (BIC)\")\n",
    "plt.show()\n",
    "\n",
    "# Choose the optimal number of clusters based on the elbow plot\n",
    "optimal_k = 3\n",
    "gmm = GaussianMixture(n_components=optimal_k, random_state=42)\n",
    "gmm.fit(df)\n",
    "\n",
    "# Add the cluster labels to the original DataFrame\n",
    "iris[\"cluster\"] = gmm.predict(df)\n",
    "print(\"Iris Dataset with Cluster Labels:\")\n",
    "print(iris.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9.4.3 Visualize the Clusters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the clusters in 2D space using the first two principal components\n",
    "pca = PCA(n_components=2)\n",
    "principal_components = pca.fit_transform(df)\n",
    "principal_df = pd.DataFrame(\n",
    "    data=principal_components,\n",
    "    columns=[\"principal_component_1\", \"principal_component_2\"],\n",
    ")\n",
    "principal_df[\"cluster\"] = iris[\"cluster\"]\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.scatterplot(\n",
    "    data=principal_df,\n",
    "    x=\"principal_component_1\",\n",
    "    y=\"principal_component_2\",\n",
    "    hue=\"cluster\",\n",
    "    palette=\"viridis\",\n",
    ")\n",
    "plt.title(\"GMM Clustering of Iris Dataset (2D PCA)\")\n",
    "plt.xlabel(\"Principal Component 1\")\n",
    "plt.ylabel(\"Principal Component 2\")\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9.4.4 Evaluate the Clustering Results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Silhouette Score\n",
    "silhouette_avg = silhouette_score(df, iris[\"cluster\"])\n",
    "print(f\"Silhouette Score: {silhouette_avg}\")\n",
    "\n",
    "# Davies-Bouldin Score\n",
    "davies_bouldin = davies_bouldin_score(df, iris[\"cluster\"])\n",
    "print(f\"Davies-Bouldin Score: {davies_bouldin}\")\n",
    "\n",
    "# Detailed cluster analysis\n",
    "print(\"Cluster Means (Centroids):\")\n",
    "print(gmm.means_)\n",
    "\n",
    "print(\"Cluster Counts:\")\n",
    "print(iris[\"cluster\"].value_counts())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Notes on Expanding GMM Analysis\n",
    "\n",
    "1. **Covariance Types**:\n",
    "\n",
    "   - Explore different covariance types (`full`, `tied`, `diag`, `spherical`) in GMM.\n",
    "   - Compare the results and choose the most suitable covariance type for your data.\n",
    "\n",
    "2. **Parameter Tuning**:\n",
    "\n",
    "   - Use grid search or other optimization techniques to find the optimal parameters for GMM, such as the number of components and covariance type.\n",
    "\n",
    "3. **Initialization Methods**:\n",
    "\n",
    "   - Compare different initialization methods (`k-means`, `random`) for the GMM algorithm.\n",
    "   - Perform multiple runs with different initializations to ensure stability and robustness of the clusters.\n",
    "\n",
    "4. **Handling High-dimensional Data**:\n",
    "\n",
    "   - Apply GMM to high-dimensional datasets and use techniques like PCA or t-SNE for visualization.\n",
    "   - Discuss how the curse of dimensionality affects the performance of GMM.\n",
    "\n",
    "5. **Comparison with Other Clustering Algorithms**:\n",
    "\n",
    "   - Compare GMM with other clustering algorithms like K-means, Hierarchical Clustering, and DBSCAN.\n",
    "   - Highlight the advantages and disadvantages of GMM, such as its flexibility in modeling different cluster shapes and its ability to provide probabilistic cluster assignments.\n",
    "\n",
    "6. **Visualizing High-dimensional Clusters**:\n",
    "\n",
    "   - Use advanced visualization techniques like t-SNE or UMAP to visualize high-dimensional data in 2D or 3D space.\n",
    "\n",
    "7. **Cluster Validation Techniques**:\n",
    "\n",
    "   - Use additional validation techniques such as the Calinski-Harabasz Index and Dunn Index to evaluate the clustering results.\n",
    "\n",
    "8. **Applying GMM to Different Datasets**:\n",
    "\n",
    "   - Apply GMM to different types of datasets, such as image data, time-series data, and text data.\n",
    "   - Discuss the challenges and considerations specific to each dataset type.\n",
    "\n",
    "9. **Outlier Detection**:\n",
    "\n",
    "   - Use GMM for outlier detection by identifying data points with low probabilities of belonging to any cluster.\n",
    "   - Compare the results with other outlier detection methods.\n",
    "\n",
    "10. **Model Selection Criteria**:\n",
    "\n",
    "- Discuss different criteria for model selection, such as the Bayesian Information Criterion (BIC) and the Akaike Information Criterion (AIC).\n",
    "- Compare their effectiveness in selecting the optimal number of components.\n",
    "\n",
    "By expanding on these topics, you can gain a deeper understanding of Gaussian Mixture Models, their applications, and limitations, and compare them with other clustering techniques.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Advanced Statistical Models\n",
    "\n",
    "### 10.1 Generalized Linear Models (GLMs)\n",
    "\n",
    "Generalized Linear Models (GLMs) are an extension of linear models that allow for response variables that have error distribution models other than a normal distribution. This section covers the steps to perform GLM analysis using different link functions and family distributions.\n",
    "\n",
    "#### 10.1.1 Introduction to GLMs\n",
    "\n",
    "##### Introduction to GLMs\n",
    "\n",
    "GLMs generalize linear regression by allowing the linear model to be related to the response variable via a link function and by allowing the response variable to follow different distributions. Common examples include logistic regression (for binary outcomes) and Poisson regression (for count data).\n",
    "\n",
    "#### 10.1.2 Logistic Regression (Binary Outcomes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate synthetic binary data\n",
    "np.random.seed(42)\n",
    "X_logistic = np.random.rand(100, 1) * 10\n",
    "y_logistic = (X_logistic.flatten() > 5).astype(int)\n",
    "\n",
    "# Convert to DataFrame\n",
    "df_logistic = pd.DataFrame({\"X\": X_logistic.flatten(), \"y\": y_logistic})\n",
    "\n",
    "# Fit a logistic regression model using GLM\n",
    "logistic_model = smf.glm(\n",
    "    formula=\"y ~ X\", data=df_logistic, family=sm.families.Binomial()\n",
    ").fit()\n",
    "print(logistic_model.summary())\n",
    "\n",
    "# Predictions\n",
    "df_logistic[\"y_pred\"] = logistic_model.predict(df_logistic[\"X\"])\n",
    "\n",
    "# Plotting the results\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.scatterplot(x=\"X\", y=\"y\", data=df_logistic, label=\"Data\", alpha=0.5)\n",
    "sns.lineplot(\n",
    "    x=\"X\",\n",
    "    y=\"y_pred\",\n",
    "    data=df_logistic,\n",
    "    color=\"red\",\n",
    "    label=\"Logistic Regression Prediction\",\n",
    ")\n",
    "plt.title(\"Logistic Regression (GLM)\")\n",
    "plt.xlabel(\"X\")\n",
    "plt.ylabel(\"Probability\")\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10.1.3 Poisson Regression (Count Data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate synthetic count data\n",
    "np.random.seed(42)\n",
    "X_poisson = np.random.rand(100, 1) * 10\n",
    "y_poisson = np.random.poisson(lam=2 + 0.3 * X_poisson.flatten(), size=100)\n",
    "\n",
    "# Convert to DataFrame\n",
    "df_poisson = pd.DataFrame({\"X\": X_poisson.flatten(), \"y\": y_poisson})\n",
    "\n",
    "# Fit a Poisson regression model using GLM\n",
    "poisson_model = smf.glm(\n",
    "    formula=\"y ~ X\", data=df_poisson, family=sm.families.Poisson()\n",
    ").fit()\n",
    "print(poisson_model.summary())\n",
    "\n",
    "# Predictions\n",
    "df_poisson[\"y_pred\"] = poisson_model.predict(df_poisson[\"X\"])\n",
    "\n",
    "# Plotting the results\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.scatterplot(x=\"X\", y=\"y\", data=df_poisson, label=\"Data\", alpha=0.5)\n",
    "sns.lineplot(\n",
    "    x=\"X\",\n",
    "    y=\"y_pred\",\n",
    "    data=df_poisson,\n",
    "    color=\"red\",\n",
    "    label=\"Poisson Regression Prediction\",\n",
    ")\n",
    "plt.title(\"Poisson Regression (GLM)\")\n",
    "plt.xlabel(\"X\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Notes on Expanding GLM Analysis\n",
    "\n",
    "1. **Different Link Functions**:\n",
    "\n",
    "   - Explore different link functions for GLMs, such as the logit link for logistic regression, the log link for Poisson regression, and the identity link for normal regression.\n",
    "   - Compare the effects of different link functions on the model fit and predictions.\n",
    "\n",
    "2. **Handling Overdispersion in Poisson Regression**:\n",
    "\n",
    "   - Address overdispersion in Poisson regression by using models like the Negative Binomial regression.\n",
    "   - Compare the fit and residuals of the Poisson and Negative Binomial models.\n",
    "\n",
    "3. **Quasi-Likelihood Models**:\n",
    "\n",
    "   - Implement quasi-likelihood models for situations where the exact distribution of the response variable is unknown.\n",
    "   - Compare the performance of quasi-likelihood models with standard GLMs.\n",
    "\n",
    "4. **Generalized Additive Models (GAMs)**:\n",
    "\n",
    "   - Extend GLMs to Generalized Additive Models (GAMs) to capture non-linear relationships between the predictors and the response variable.\n",
    "   - Implement and compare GAMs with GLMs on various datasets.\n",
    "\n",
    "5. **Real-world Datasets**:\n",
    "\n",
    "   - Apply GLMs to real-world datasets, such as health data (e.g., predicting disease incidence), insurance data (e.g., predicting claim counts), and marketing data (e.g., predicting purchase probability).\n",
    "   - Discuss the practical challenges and considerations when applying GLMs to real-world data.\n",
    "\n",
    "6. **Model Diagnostics and Validation**:\n",
    "   - Perform model diagnostics to assess the goodness-of-fit of GLMs, including residual analysis and influence diagnostics.\n",
    "   - Implement cross-validation techniques to evaluate the model performance on unseen data.\n",
    "\n",
    "<!-- By expanding on these topics, you can gain a deeper understanding of Generalized Linear Models, their applications, and limitations, and explore more advanced extensions and use cases. -->\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10.2 Survival Analysis\n",
    "\n",
    "Survival analysis is a branch of statistics that deals with the analysis of time-to-event data. Common applications include clinical trials, reliability engineering, and event history analysis. This section covers the Kaplan-Meier estimator and the Cox proportional hazards model.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10.2.1 Kaplan-Meier Estimator\n",
    "\n",
    "The Kaplan-Meier estimator is a non-parametric statistic used to estimate the survival function from lifetime data. It provides a step function that represents the probability of surviving past a certain time point.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lifelines import KaplanMeierFitter\n",
    "\n",
    "# Generate synthetic survival data\n",
    "np.random.seed(42)\n",
    "durations = np.random.exponential(scale=10, size=100)  # Survival times\n",
    "event_observed = np.random.binomial(\n",
    "    n=1, p=0.7, size=100\n",
    ")  # Censoring indicator (1 if event occurred, 0 if censored)\n",
    "\n",
    "# Fit the Kaplan-Meier estimator\n",
    "kmf = KaplanMeierFitter()\n",
    "kmf.fit(durations, event_observed)\n",
    "\n",
    "# Plot the survival function\n",
    "plt.figure(figsize=(10, 6))\n",
    "kmf.plot_survival_function()\n",
    "plt.title(\"Kaplan-Meier Estimate of Survival Function\")\n",
    "plt.xlabel(\"Time\")\n",
    "plt.ylabel(\"Survival Probability\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10.2.2 Cox Proportional Hazards Model\n",
    "\n",
    "The Cox proportional hazards model is a semi-parametric model used to estimate the hazard (or risk) of an event occurring, given certain predictor variables. It assumes that the hazard ratios are constant over time.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lifelines import CoxPHFitter\n",
    "\n",
    "# Generate synthetic data with covariates\n",
    "np.random.seed(42)\n",
    "data = pd.DataFrame(\n",
    "    {\n",
    "        \"duration\": np.random.exponential(scale=10, size=100),\n",
    "        \"event\": np.random.binomial(n=1, p=0.7, size=100),\n",
    "        \"age\": np.random.randint(20, 70, size=100),\n",
    "        \"treatment\": np.random.binomial(n=1, p=0.5, size=100),\n",
    "    }\n",
    ")\n",
    "\n",
    "# Fit the Cox proportional hazards model\n",
    "cph = CoxPHFitter()\n",
    "cph.fit(data, duration_col=\"duration\", event_col=\"event\")\n",
    "\n",
    "# Print the summary of the Cox model\n",
    "print(cph.summary)\n",
    "\n",
    "# Plot the survival function for different covariate values\n",
    "plt.figure(figsize=(10, 6))\n",
    "cph.plot_partial_effects_on_outcome(\n",
    "    covariates=\"treatment\", values=[0, 1], cmap=\"coolwarm\"\n",
    ")\n",
    "plt.title(\"Survival Function for Different Treatment Groups\")\n",
    "plt.xlabel(\"Time\")\n",
    "plt.ylabel(\"Survival Probability\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Notes on Expanding Survival Analysis\n",
    "\n",
    "1. **Advanced Kaplan-Meier Analysis**:\n",
    "\n",
    "   - Stratify the Kaplan-Meier curves by different groups (e.g., treatment vs. control) and perform log-rank tests to compare the survival curves.\n",
    "   - Handle tied survival times using appropriate methods.\n",
    "\n",
    "2. **Handling Censoring and Truncation**:\n",
    "\n",
    "   - Discuss different types of censoring (right, left, interval) and truncation (left, right) and how to handle them in survival analysis.\n",
    "   - Use interval-censored and left-truncated data with appropriate models.\n",
    "\n",
    "3. **Extended Cox Models**:\n",
    "\n",
    "   - Explore time-dependent covariates and interaction terms in the Cox model.\n",
    "   - Use penalized Cox models (e.g., Lasso, Ridge) to handle high-dimensional data.\n",
    "\n",
    "4. **Alternative Survival Models**:\n",
    "\n",
    "   - Implement other survival models such as the Weibull, exponential, and log-normal models.\n",
    "   - Compare parametric survival models with the semi-parametric Cox model.\n",
    "\n",
    "5. **Model Diagnostics and Validation**:\n",
    "\n",
    "   - Perform diagnostic checks for the Cox model, such as checking the proportional hazards assumption and examining residuals.\n",
    "   - Use cross-validation and bootstrapping techniques to assess model performance.\n",
    "\n",
    "6. **Real-world Datasets**:\n",
    "   - Apply survival analysis to real-world datasets, such as clinical trial data, customer churn data, and reliability data.\n",
    "   - Discuss practical challenges and considerations when working with survival data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10.3 Bayesian Statistics\n",
    "\n",
    "Bayesian statistics is a paradigm that allows us to update our beliefs about parameters using data. This section covers Bayesian inference and Markov Chain Monte Carlo (MCMC) methods.\n",
    "\n",
    "### 10.3.1 Bayesian Inference\n",
    "\n",
    "### Bayesian Inference\n",
    "\n",
    "Bayesian inference involves updating the probability distribution of a parameter based on new data. This section demonstrates how to perform Bayesian inference using PyMC3.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bayesian Inference\n",
    "\n",
    "import pymc3 as pm\n",
    "import arviz as az\n",
    "\n",
    "# Generate synthetic data\n",
    "np.random.seed(42)\n",
    "data = np.random.normal(loc=5, scale=2, size=100)\n",
    "\n",
    "# Define the Bayesian model\n",
    "with pm.Model() as model:\n",
    "    # Prior distributions for unknown parameters\n",
    "    mu = pm.Normal(\"mu\", mu=0, sigma=10)\n",
    "    sigma = pm.HalfNormal(\"sigma\", sigma=1)\n",
    "\n",
    "    # Likelihood (sampling distribution) of observations\n",
    "    likelihood = pm.Normal(\"likelihood\", mu=mu, sigma=sigma, observed=data)\n",
    "\n",
    "    # Posterior distribution\n",
    "    trace = pm.sample(2000, return_inferencedata=True)\n",
    "\n",
    "# Summary of the posterior\n",
    "print(az.summary(trace, hdi_prob=0.95))\n",
    "\n",
    "# Plot posterior distributions\n",
    "az.plot_posterior(trace, hdi_prob=0.95)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10.3.2 Markov Chain Monte Carlo (MCMC)\n",
    "\n",
    "MCMC methods are used to sample from complex posterior distributions. This section demonstrates how to perform MCMC using the Metropolis-Hastings algorithm.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10.3.3 Comparing Different MCMC Algorithms\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparing Different MCMC Algorithms\n",
    "\n",
    "Compare the results of different MCMC algorithms such as Metropolis-Hastings, NUTS (No-U-Turn Sampler), and Gibbs sampling.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Notes on Expanding Bayesian Analysis\n",
    "\n",
    "1. **Prior Sensitivity Analysis**:\n",
    "\n",
    "   - Perform sensitivity analysis to understand the impact of different priors on the posterior distribution.\n",
    "   - Compare results with different priors (e.g., informative vs. non-informative priors).\n",
    "\n",
    "2. **Hierarchical Bayesian Models**:\n",
    "\n",
    "   - Extend the analysis to hierarchical models, where parameters are nested and have their own priors.\n",
    "   - Apply hierarchical models to real-world datasets such as clinical trials or multi-level marketing data.\n",
    "\n",
    "3. **Bayesian Model Comparison**:\n",
    "\n",
    "   - Use model comparison techniques such as Bayes factors, WAIC (Watanabe-Akaike Information Criterion), and LOO (Leave-One-Out) cross-validation to compare different models.\n",
    "   - Evaluate the trade-offs between model complexity and fit.\n",
    "\n",
    "4. **Convergence Diagnostics**:\n",
    "\n",
    "   - Assess the convergence of MCMC chains using diagnostics such as trace plots, autocorrelation plots, and the Gelman-Rubin statistic (R-hat).\n",
    "   - Ensure sufficient mixing and convergence of the chains.\n",
    "\n",
    "5. **Advanced MCMC Techniques**:\n",
    "\n",
    "   - Explore advanced MCMC techniques such as Hamiltonian Monte Carlo (HMC), Particle MCMC, and Variational Inference.\n",
    "   - Compare their performance and applicability to different types of problems.\n",
    "\n",
    "6. **Real-world Applications**:\n",
    "\n",
    "   - Apply Bayesian inference and MCMC methods to real-world problems in various domains such as finance (e.g., risk modeling), healthcare (e.g., disease progression), and marketing (e.g., customer segmentation).\n",
    "   - Discuss practical considerations and challenges in implementing Bayesian methods.\n",
    "\n",
    "7. **Software and Tools**:\n",
    "   - Introduce other Bayesian modeling software and tools such as Stan, JAGS, and Turing.jl.\n",
    "   - Compare their features, capabilities, and ease of use with PyMC3.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Resampling Methods\n",
    "\n",
    "### 11.1 Bootstrap\n",
    "\n",
    "Bootstrap is a powerful statistical method that involves repeatedly resampling with replacement from a dataset to estimate the distribution of a statistic. This section covers the steps to perform bootstrap resampling and interpret its results.\n",
    "\n",
    "### 11.1.1 Introduction to Bootstrap\n",
    "\n",
    "Bootstrap resampling allows us to estimate the sampling distribution of a statistic by repeatedly sampling with replacement from the observed data. It is particularly useful for estimating confidence intervals and assessing the variability of a statistic.\n",
    "\n",
    "### 11.1.2 Bootstrap Resampling\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bootstrap Resampling\n",
    "\n",
    "# Generate synthetic data\n",
    "np.random.seed(42)\n",
    "data = np.random.normal(loc=50, scale=10, size=100)\n",
    "\n",
    "# Number of bootstrap samples\n",
    "n_bootstrap = 1000\n",
    "\n",
    "\n",
    "# Function to calculate the statistic of interest (e.g., mean)\n",
    "def bootstrap_statistic(data, n_bootstrap, statistic_func):\n",
    "    bootstrap_samples = np.random.choice(data, (n_bootstrap, len(data)), replace=True)\n",
    "    bootstrap_statistics = np.apply_along_axis(statistic_func, 1, bootstrap_samples)\n",
    "    return bootstrap_statistics\n",
    "\n",
    "\n",
    "# Calculate bootstrap statistics (mean in this case)\n",
    "bootstrap_means = bootstrap_statistic(data, n_bootstrap, np.mean)\n",
    "\n",
    "# Summary statistics of the bootstrap distribution\n",
    "print(f\"Bootstrap Mean: {np.mean(bootstrap_means)}\")\n",
    "print(f\"Bootstrap Standard Deviation: {np.std(bootstrap_means)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 11.1.3 Confidence Intervals using Bootstrap\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the 95% confidence interval for the mean\n",
    "ci_lower, ci_upper = np.percentile(bootstrap_means, [2.5, 97.5])\n",
    "print(f\"95% Confidence Interval for the Mean: [{ci_lower}, {ci_upper}]\")\n",
    "\n",
    "# Plot the bootstrap distribution and confidence interval\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.histplot(bootstrap_means, kde=True)\n",
    "plt.axvline(ci_lower, color=\"red\", linestyle=\"--\", label=\"Lower 95% CI\")\n",
    "plt.axvline(ci_upper, color=\"red\", linestyle=\"--\", label=\"Upper 95% CI\")\n",
    "plt.title(\"Bootstrap Distribution of the Mean\")\n",
    "plt.xlabel(\"Mean\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 11.1.4 Bootstrap for Other Statistics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate bootstrap statistics for the median\n",
    "bootstrap_medians = bootstrap_statistic(data, n_bootstrap, np.median)\n",
    "\n",
    "# Summary statistics of the bootstrap distribution for the median\n",
    "print(f\"Bootstrap Median: {np.mean(bootstrap_medians)}\")\n",
    "print(f\"Bootstrap Standard Deviation: {np.std(bootstrap_medians)}\")\n",
    "\n",
    "# Calculate the 95% confidence interval for the median\n",
    "ci_lower_median, ci_upper_median = np.percentile(bootstrap_medians, [2.5, 97.5])\n",
    "print(f\"95% Confidence Interval for the Median: [{ci_lower_median}, {ci_upper_median}]\")\n",
    "\n",
    "# Plot the bootstrap distribution and confidence interval for the median\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.histplot(bootstrap_medians, kde=True)\n",
    "plt.axvline(ci_lower_median, color=\"red\", linestyle=\"--\", label=\"Lower 95% CI\")\n",
    "plt.axvline(ci_upper_median, color=\"red\", linestyle=\"--\", label=\"Upper 95% CI\")\n",
    "plt.title(\"Bootstrap Distribution of the Median\")\n",
    "plt.xlabel(\"Median\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Notes on Expanding Bootstrap Analysis\n",
    "\n",
    "1. **Bias-Corrected and Accelerated (BCa) Bootstrap**:\n",
    "\n",
    "   - Implement BCa bootstrap to account for bias and skewness in the bootstrap distribution.\n",
    "   - Compare the BCa confidence intervals with the standard percentile method.\n",
    "\n",
    "2. **Bootstrap for Regression Models**:\n",
    "\n",
    "   - Apply bootstrap resampling to regression models to estimate the variability of regression coefficients.\n",
    "   - Compare the bootstrap estimates with traditional methods such as standard errors.\n",
    "\n",
    "3. **Bootstrap Hypothesis Testing**:\n",
    "\n",
    "   - Use bootstrap to perform hypothesis testing, such as testing the difference between two means or medians.\n",
    "   - Compare the results with traditional parametric tests.\n",
    "\n",
    "4. **Non-parametric Bootstrap**:\n",
    "\n",
    "   - Discuss the use of non-parametric bootstrap methods when the data does not meet the assumptions of parametric methods.\n",
    "   - Apply non-parametric bootstrap to various datasets.\n",
    "\n",
    "5. **Applications to Real-world Data**:\n",
    "\n",
    "   - Apply bootstrap methods to real-world datasets, such as finance (e.g., estimating portfolio risk), healthcare (e.g., estimating treatment effects), and marketing (e.g., estimating customer lifetime value).\n",
    "   - Discuss practical challenges and considerations when implementing bootstrap methods.\n",
    "\n",
    "6. **Advanced Bootstrap Techniques**:\n",
    "   - Explore advanced bootstrap techniques such as the smoothed bootstrap, block bootstrap for time-series data, and the parametric bootstrap.\n",
    "   - Compare their performance and applicability to different types of problems.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11.2 Cross-validation Techniques\n",
    "\n",
    "Cross-validation is a resampling method used to evaluate the performance of a model on a limited dataset. It helps to assess how the model generalizes to an independent dataset. This section covers k-fold cross-validation, stratified k-fold cross-validation, and leave-one-out cross-validation (LOOCV).\n",
    "\n",
    "### 11.2.1 K-fold Cross-validation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold, cross_val_score\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# Generate synthetic data\n",
    "np.random.seed(42)\n",
    "X = np.random.rand(100, 1) * 10\n",
    "y = 2 * X.flatten() + np.random.randn(100) * 2\n",
    "\n",
    "# Initialize the model\n",
    "model = LinearRegression()\n",
    "\n",
    "# Perform k-fold cross-validation\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "cv_scores = cross_val_score(model, X, y, cv=kf, scoring=\"neg_mean_squared_error\")\n",
    "\n",
    "# Print the cross-validation scores\n",
    "print(f\"K-fold Cross-validation Scores: {-cv_scores}\")\n",
    "print(f\"Mean Cross-validation Score: {-cv_scores.mean()}\")\n",
    "print(f\"Standard Deviation of Cross-validation Scores: {cv_scores.std()}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 11.2.2 Stratified K-fold Cross-validation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.datasets import make_classification\n",
    "\n",
    "# Generate synthetic classification data\n",
    "X_class, y_class = make_classification(\n",
    "    n_samples=100, n_features=5, n_informative=3, n_redundant=2, random_state=42\n",
    ")\n",
    "\n",
    "# Initialize the model\n",
    "model_class = LogisticRegression()\n",
    "\n",
    "# Perform stratified k-fold cross-validation\n",
    "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "cv_scores_class = cross_val_score(\n",
    "    model_class, X_class, y_class, cv=skf, scoring=\"accuracy\"\n",
    ")\n",
    "\n",
    "# Print the cross-validation scores\n",
    "print(f\"Stratified K-fold Cross-validation Scores: {cv_scores_class}\")\n",
    "print(f\"Mean Cross-validation Score: {cv_scores_class.mean()}\")\n",
    "print(f\"Standard Deviation of Cross-validation Scores: {cv_scores_class.std()}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 11.2.3 Leave-one-out Cross-validation (LOOCV)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import LeaveOneOut\n",
    "\n",
    "# Initialize the model\n",
    "model_loocv = LinearRegression()\n",
    "\n",
    "# Perform leave-one-out cross-validation\n",
    "loo = LeaveOneOut()\n",
    "cv_scores_loocv = cross_val_score(\n",
    "    model_loocv, X, y, cv=loo, scoring=\"neg_mean_squared_error\"\n",
    ")\n",
    "\n",
    "# Print the cross-validation scores\n",
    "print(f\"LOOCV Scores: {-cv_scores_loocv}\")\n",
    "print(f\"Mean LOOCV Score: {-cv_scores_loocv.mean()}\")\n",
    "print(f\"Standard Deviation of LOOCV Scores: {cv_scores_loocv.std()}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Notes on Expanding Cross-validation Techniques\n",
    "\n",
    "1. **Nested Cross-validation**:\n",
    "\n",
    "   - Implement nested cross-validation for hyperparameter tuning and model evaluation.\n",
    "   - Compare nested cross-validation results with standard cross-validation.\n",
    "\n",
    "2. **Repeated Cross-validation**:\n",
    "\n",
    "   - Use repeated k-fold cross-validation to reduce variability in the performance estimate.\n",
    "   - Perform multiple iterations of k-fold cross-validation and average the results.\n",
    "\n",
    "3. **Cross-validation for Different Models**:\n",
    "\n",
    "   - Apply cross-validation techniques to different models (e.g., decision trees, support vector machines, neural networks).\n",
    "   - Compare the performance of different models using cross-validation scores.\n",
    "\n",
    "4. **Handling Imbalanced Data**:\n",
    "\n",
    "   - Implement stratified k-fold cross-validation for handling imbalanced datasets.\n",
    "   - Discuss other techniques for dealing with imbalanced data, such as SMOTE or ADASYN.\n",
    "\n",
    "5. **Time Series Cross-validation**:\n",
    "\n",
    "   - Apply time series-specific cross-validation techniques, such as rolling cross-validation or expanding window cross-validation.\n",
    "   - Compare time series cross-validation with standard cross-validation techniques.\n",
    "\n",
    "6. **Cross-validation in Practice**:\n",
    "\n",
    "   - Apply cross-validation techniques to real-world datasets in various domains, such as healthcare, finance, and marketing.\n",
    "   - Discuss practical considerations and challenges when implementing cross-validation.\n",
    "\n",
    "7. **Visualization of Cross-validation Results**:\n",
    "   - Visualize cross-validation results using box plots, violin plots, or other suitable methods.\n",
    "   - Analyze and interpret the variability and distribution of cross-validation scores.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Statistical Machine Learning\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 12.1 Decision Trees\n",
    "\n",
    "Decision trees are a non-parametric supervised learning method used for classification and regression tasks. They work by splitting the data into subsets based on the value of input features, creating a tree-like structure. This section covers the steps to build, visualize, and evaluate decision trees.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 12.1.1 Building a Decision Tree Classifier\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "\n",
    "# Load the Iris dataset\n",
    "iris = load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.3, random_state=42\n",
    ")\n",
    "\n",
    "# Initialize and train the decision tree classifier\n",
    "clf = DecisionTreeClassifier(random_state=42)\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "print(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred))\n",
    "print(\"\\nConfusion Matrix:\\n\", confusion_matrix(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 12.1.2 Visualizing the Decision Tree\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import plot_tree\n",
    "\n",
    "# Plot the decision tree\n",
    "plt.figure(figsize=(15, 10))\n",
    "plot_tree(\n",
    "    clf, feature_names=iris.feature_names, class_names=iris.target_names, filled=True\n",
    ")\n",
    "plt.title(\"Decision Tree Visualization\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 12.1.3 Hyperparameter Tuning\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Define the parameter grid\n",
    "param_grid = {\n",
    "    \"criterion\": [\"gini\", \"entropy\"],\n",
    "    \"max_depth\": [None, 10, 20, 30, 40, 50],\n",
    "    \"min_samples_split\": [2, 5, 10],\n",
    "    \"min_samples_leaf\": [1, 2, 4],\n",
    "}\n",
    "\n",
    "# Initialize GridSearchCV\n",
    "grid_search = GridSearchCV(\n",
    "    estimator=clf, param_grid=param_grid, cv=5, n_jobs=-1, verbose=2\n",
    ")\n",
    "\n",
    "# Fit the grid search to the data\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Print the best parameters and best score\n",
    "print(f\"Best Parameters: {grid_search.best_params_}\")\n",
    "print(f\"Best Score: {grid_search.best_score_}\")\n",
    "\n",
    "# Train the model with the best parameters\n",
    "best_clf = grid_search.best_estimator_\n",
    "best_clf.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions and evaluate the model\n",
    "y_pred_best = best_clf.predict(X_test)\n",
    "accuracy_best = accuracy_score(y_test, y_pred_best)\n",
    "print(f\"Accuracy with Best Parameters: {accuracy_best}\")\n",
    "print(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred_best))\n",
    "print(\"\\nConfusion Matrix:\\n\", confusion_matrix(y_test, y_pred_best))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Notes on Expanding Decision Trees Analysis\n",
    "\n",
    "1. **Feature Importance**:\n",
    "\n",
    "   - Analyze the feature importance scores to understand which features are most influential in the decision tree model.\n",
    "   - Visualize feature importance using bar plots.\n",
    "\n",
    "2. **Pruning Techniques**:\n",
    "\n",
    "   - Implement pruning techniques to avoid overfitting, such as cost complexity pruning (CCP) or reduced error pruning.\n",
    "   - Compare the performance of pruned and unpruned trees.\n",
    "\n",
    "3. **Handling Imbalanced Data**:\n",
    "\n",
    "   - Use techniques to handle imbalanced datasets, such as class weighting, oversampling, or undersampling.\n",
    "   - Evaluate the impact of these techniques on the decision tree performance.\n",
    "\n",
    "4. **Comparison with Other Models**:\n",
    "\n",
    "   - Compare the performance of decision trees with other machine learning models, such as logistic regression, random forests, and support vector machines.\n",
    "   - Discuss the advantages and disadvantages of decision trees in different scenarios.\n",
    "\n",
    "5. **Advanced Decision Tree Algorithms**:\n",
    "\n",
    "   - Explore advanced decision tree algorithms, such as C4.5, CART (Classification and Regression Trees), and CHAID (Chi-square Automatic Interaction Detector).\n",
    "   - Compare their performance and applicability to different types of problems.\n",
    "\n",
    "6. **Real-world Applications**:\n",
    "\n",
    "   - Apply decision tree models to real-world datasets, such as healthcare (e.g., predicting disease outcomes), finance (e.g., credit risk assessment), and marketing (e.g., customer segmentation).\n",
    "   - Discuss practical challenges and considerations when implementing decision trees in real-world scenarios.\n",
    "\n",
    "7. **Ensemble Methods**:\n",
    "   - Introduce ensemble methods that combine multiple decision trees, such as bagging (Bootstrap Aggregating), random forests, and boosting (e.g., AdaBoost, Gradient Boosting).\n",
    "   - Compare the performance of ensemble methods with single decision trees.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12.2 Random Forests\n",
    "\n",
    "Random forests are an ensemble learning method that combines multiple decision trees to improve predictive performance and control overfitting. This section covers the steps to build, visualize, and evaluate random forests.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 12.2.1 Building a Random Forest Classifier\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Building a Random Forest Classifier\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "\n",
    "# Load the Iris dataset\n",
    "iris = load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.3, random_state=42\n",
    ")\n",
    "\n",
    "# Initialize and train the random forest classifier\n",
    "rf_clf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "rf_clf.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = rf_clf.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "print(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred))\n",
    "print(\"\\nConfusion Matrix:\\n\", confusion_matrix(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 12.2.2 Feature Importance\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature Importance\n",
    "\n",
    "# Get feature importance scores\n",
    "feature_importances = rf_clf.feature_importances_\n",
    "features = iris.feature_names\n",
    "\n",
    "# Create a DataFrame for better visualization\n",
    "importances_df = pd.DataFrame({\"Feature\": features, \"Importance\": feature_importances})\n",
    "\n",
    "# Sort the DataFrame by importance\n",
    "importances_df = importances_df.sort_values(by=\"Importance\", ascending=False)\n",
    "\n",
    "# Plot the feature importances\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.barplot(x=\"Importance\", y=\"Feature\", data=importances_df)\n",
    "plt.title(\"Feature Importance in Random Forest\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 12.2.3 Hyperparameter Tuning\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Define the parameter grid\n",
    "param_grid = {\n",
    "    \"n_estimators\": [50, 100, 200],\n",
    "    \"max_features\": [\"auto\", \"sqrt\", \"log2\"],\n",
    "    \"max_depth\": [None, 10, 20, 30],\n",
    "    \"min_samples_split\": [2, 5, 10],\n",
    "    \"min_samples_leaf\": [1, 2, 4],\n",
    "}\n",
    "\n",
    "# Initialize GridSearchCV\n",
    "grid_search = GridSearchCV(\n",
    "    estimator=rf_clf, param_grid=param_grid, cv=5, n_jobs=-1, verbose=2\n",
    ")\n",
    "\n",
    "# Fit the grid search to the data\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Print the best parameters and best score\n",
    "print(f\"Best Parameters: {grid_search.best_params_}\")\n",
    "print(f\"Best Score: {grid_search.best_score_}\")\n",
    "\n",
    "# Train the model with the best parameters\n",
    "best_rf_clf = grid_search.best_estimator_\n",
    "best_rf_clf.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions and evaluate the model\n",
    "y_pred_best = best_rf_clf.predict(X_test)\n",
    "accuracy_best = accuracy_score(y_test, y_pred_best)\n",
    "print(f\"Accuracy with Best Parameters: {accuracy_best}\")\n",
    "print(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred_best))\n",
    "print(\"\\nConfusion Matrix:\\n\", confusion_matrix(y_test, y_pred_best))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Notes on Expanding Random Forests Analysis\n",
    "\n",
    "1. **Out-of-Bag (OOB) Error**:\n",
    "\n",
    "   - Use the Out-of-Bag (OOB) error estimate to evaluate the generalization error of the random forest model.\n",
    "   - Enable OOB estimation in the RandomForestClassifier and compare it with cross-validation results.\n",
    "\n",
    "2. **Handling Imbalanced Data**:\n",
    "\n",
    "   - Implement techniques to handle imbalanced datasets, such as class weighting, oversampling, or undersampling.\n",
    "   - Evaluate the impact of these techniques on the random forest performance.\n",
    "\n",
    "3. **Comparison with Other Models**:\n",
    "\n",
    "   - Compare the performance of random forests with other machine learning models, such as decision trees, support vector machines, and gradient boosting.\n",
    "   - Discuss the advantages and disadvantages of random forests in different scenarios.\n",
    "\n",
    "4. **Interpretability**:\n",
    "\n",
    "   - Use techniques like SHAP (SHapley Additive exPlanations) values to interpret the predictions of the random forest model.\n",
    "   - Visualize the SHAP values to understand the contribution of each feature to the predictions.\n",
    "\n",
    "5. **Real-world Applications**:\n",
    "\n",
    "   - Apply random forest models to real-world datasets, such as healthcare (e.g., predicting disease outcomes), finance (e.g., credit risk assessment), and marketing (e.g., customer segmentation).\n",
    "   - Discuss practical challenges and considerations when implementing random forests in real-world scenarios.\n",
    "\n",
    "6. **Ensemble Methods**:\n",
    "\n",
    "   - Explore other ensemble methods that can be combined with random forests, such as stacking, bagging, and boosting.\n",
    "   - Compare the performance of these ensemble methods with standalone random forests.\n",
    "\n",
    "7. **Time Series Data**:\n",
    "   - Adapt random forests for time series data by using lagged features or combining them with other time series models.\n",
    "   - Evaluate the performance of random forests on time series forecasting tasks.\n",
    "\n",
    "<!-- By expanding on these topics, you can gain a deeper understanding of random forests, their applications, and limitations, and explore more advanced techniques and use cases. -->\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12.3 Support Vector Machines (SVM)\n",
    "\n",
    "Support Vector Machines (SVM) are supervised learning models used for classification and regression tasks. SVMs are effective in high-dimensional spaces and are known for their use of the kernel trick to handle non-linearly separable data. This section covers the steps to build, visualize, and evaluate SVM models.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 12.3.1 Building a Support Vector Classifier\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "\n",
    "# Load the Iris dataset\n",
    "iris = load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.3, random_state=42\n",
    ")\n",
    "\n",
    "# Initialize and train the SVM classifier\n",
    "svm_clf = SVC(kernel=\"linear\", random_state=42)\n",
    "svm_clf.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = svm_clf.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "print(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred))\n",
    "print(\"\\nConfusion Matrix:\\n\", confusion_matrix(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 12.3.2 Visualizing the Decision Boundary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Reduce dimensions to 2D for visualization\n",
    "pca = PCA(n_components=2)\n",
    "X_train_2d = pca.fit_transform(X_train)\n",
    "X_test_2d = pca.transform(X_test)\n",
    "\n",
    "# Train the SVM classifier on the reduced data\n",
    "svm_clf_2d = SVC(kernel=\"linear\", random_state=42)\n",
    "svm_clf_2d.fit(X_train_2d, y_train)\n",
    "\n",
    "# Create a mesh grid for plotting the decision boundary\n",
    "h = 0.02  # step size in the mesh\n",
    "x_min, x_max = X_train_2d[:, 0].min() - 1, X_train_2d[:, 0].max() + 1\n",
    "y_min, y_max = X_train_2d[:, 1].min() - 1, X_train_2d[:, 1].max() + 1\n",
    "xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))\n",
    "\n",
    "# Plot the decision boundary\n",
    "Z = svm_clf_2d.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "Z = Z.reshape(xx.shape)\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.contourf(xx, yy, Z, alpha=0.8)\n",
    "plt.scatter(\n",
    "    X_train_2d[:, 0],\n",
    "    X_train_2d[:, 1],\n",
    "    c=y_train,\n",
    "    edgecolors=\"k\",\n",
    "    marker=\"o\",\n",
    "    label=\"Training Data\",\n",
    ")\n",
    "plt.scatter(\n",
    "    X_test_2d[:, 0],\n",
    "    X_test_2d[:, 1],\n",
    "    c=y_test,\n",
    "    edgecolors=\"k\",\n",
    "    marker=\"x\",\n",
    "    label=\"Testing Data\",\n",
    ")\n",
    "plt.title(\"SVM Decision Boundary with Linear Kernel\")\n",
    "plt.xlabel(\"Principal Component 1\")\n",
    "plt.ylabel(\"Principal Component 2\")\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 12.3.3 Hyperparameter Tuning\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameter Tuning\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Define the parameter grid\n",
    "param_grid = {\n",
    "    \"C\": [0.1, 1, 10, 100],\n",
    "    \"gamma\": [\"scale\", \"auto\"],\n",
    "    \"kernel\": [\"linear\", \"rbf\", \"poly\"],\n",
    "}\n",
    "\n",
    "# Initialize GridSearchCV\n",
    "grid_search = GridSearchCV(\n",
    "    estimator=SVC(), param_grid=param_grid, cv=5, n_jobs=-1, verbose=2\n",
    ")\n",
    "\n",
    "# Fit the grid search to the data\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Print the best parameters and best score\n",
    "print(f\"Best Parameters: {grid_search.best_params_}\")\n",
    "print(f\"Best Score: {grid_search.best_score_}\")\n",
    "\n",
    "# Train the model with the best parameters\n",
    "best_svm_clf = grid_search.best_estimator_\n",
    "best_svm_clf.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions and evaluate the model\n",
    "y_pred_best = best_svm_clf.predict(X_test)\n",
    "accuracy_best = accuracy_score(y_test, y_pred_best)\n",
    "print(f\"Accuracy with Best Parameters: {accuracy_best}\")\n",
    "print(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred_best))\n",
    "print(\"\\nConfusion Matrix:\\n\", confusion_matrix(y_test, y_pred_best))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Notes on Expanding SVM Analysis\n",
    "\n",
    "1. **Kernel Trick**:\n",
    "\n",
    "   - Explore different kernels (linear, polynomial, radial basis function (RBF), sigmoid) and their impact on model performance.\n",
    "   - Implement custom kernels and compare their performance with standard kernels.\n",
    "\n",
    "2. **Handling Imbalanced Data**:\n",
    "\n",
    "   - Use techniques to handle imbalanced datasets, such as class weighting, oversampling, or undersampling.\n",
    "   - Evaluate the impact of these techniques on the SVM performance.\n",
    "\n",
    "3. **SVM for Regression**:\n",
    "\n",
    "   - Implement Support Vector Regression (SVR) for continuous target variables.\n",
    "   - Compare the performance of SVR with other regression models.\n",
    "\n",
    "4. **Multiclass Classification**:\n",
    "\n",
    "   - Explore different strategies for multiclass classification with SVMs, such as one-vs-one and one-vs-all.\n",
    "   - Evaluate the performance of these strategies on multiclass datasets.\n",
    "\n",
    "5. **Comparison with Other Models**:\n",
    "\n",
    "   - Compare the performance of SVMs with other machine learning models, such as logistic regression, decision trees, and random forests.\n",
    "   - Discuss the advantages and disadvantages of SVMs in different scenarios.\n",
    "\n",
    "6. **Scalability and Efficiency**:\n",
    "\n",
    "   - Discuss the scalability and efficiency of SVMs for large datasets.\n",
    "   - Implement techniques to improve the scalability of SVMs, such as using stochastic gradient descent (SGD) or approximate SVM algorithms.\n",
    "\n",
    "7. **Real-world Applications**:\n",
    "   - Apply SVM models to real-world datasets, such as image classification, text classification, and bioinformatics.\n",
    "   - Discuss practical challenges and considerations when implementing SVMs in real-world scenarios.\n",
    "\n",
    "<!-- By expanding on these topics, you can gain a deeper understanding of Support Vector Machines, their applications, and limitations, and explore more advanced techniques and use cases. -->\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12.4 Neural Networks\n",
    "\n",
    "Neural networks are a set of algorithms modeled after the human brain that are designed to recognize patterns. They are widely used for classification, regression, and many other tasks. This section covers the steps to build, train, and evaluate neural networks using TensorFlow and Keras.\n",
    "\n",
    "### 12.4.1 Building a Neural Network Classifier\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Building a Neural Network Classifier\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "\n",
    "# Load the Iris dataset\n",
    "iris = load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.3, random_state=42\n",
    ")\n",
    "\n",
    "# Standardize the data\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# One-hot encode the target variable\n",
    "encoder = OneHotEncoder(sparse=False)\n",
    "y_train_encoded = encoder.fit_transform(y_train.reshape(-1, 1))\n",
    "y_test_encoded = encoder.transform(y_test.reshape(-1, 1))\n",
    "\n",
    "# Initialize the neural network model\n",
    "model = Sequential()\n",
    "model.add(Dense(12, input_dim=X_train.shape[1], activation=\"relu\"))\n",
    "model.add(Dense(8, activation=\"relu\"))\n",
    "model.add(Dense(y_train_encoded.shape[1], activation=\"softmax\"))\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer=\"adam\", loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(\n",
    "    X_train, y_train_encoded, epochs=100, batch_size=10, validation_split=0.2, verbose=2\n",
    ")\n",
    "\n",
    "# Evaluate the model\n",
    "loss, accuracy = model.evaluate(X_test, y_test_encoded)\n",
    "print(f\"Test Accuracy: {accuracy}\")\n",
    "\n",
    "# Make predictions\n",
    "y_pred_encoded = model.predict(X_test)\n",
    "y_pred = encoder.inverse_transform(y_pred_encoded)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "print(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred))\n",
    "print(\"\\nConfusion Matrix:\\n\", confusion_matrix(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 12.4.2 Visualizing the Training Process\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizing the Training Process\n",
    "\n",
    "# Plot training & validation accuracy values\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(history.history[\"accuracy\"])\n",
    "plt.plot(history.history[\"val_accuracy\"])\n",
    "plt.title(\"Model Accuracy\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.legend([\"Train\", \"Validation\"], loc=\"upper left\")\n",
    "plt.show()\n",
    "\n",
    "# Plot training & validation loss values\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(history.history[\"loss\"])\n",
    "plt.plot(history.history[\"val_loss\"])\n",
    "plt.title(\"Model Loss\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.legend([\"Train\", \"Validation\"], loc=\"upper left\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 12.4.3 Hyperparameter Tuning\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameter Tuning\n",
    "from kerastuner.tuners import RandomSearch\n",
    "\n",
    "\n",
    "# Define the model-building function for Keras Tuner\n",
    "def build_model(hp):\n",
    "    model = Sequential()\n",
    "    model.add(\n",
    "        Dense(\n",
    "            units=hp.Int(\"units_input\", min_value=8, max_value=32, step=8),\n",
    "            activation=\"relu\",\n",
    "            input_dim=X_train.shape[1],\n",
    "        )\n",
    "    )\n",
    "    for i in range(hp.Int(\"num_layers\", 1, 3)):\n",
    "        model.add(\n",
    "            Dense(\n",
    "                units=hp.Int(f\"units_{i}\", min_value=8, max_value=32, step=8),\n",
    "                activation=\"relu\",\n",
    "            )\n",
    "        )\n",
    "    model.add(Dense(y_train_encoded.shape[1], activation=\"softmax\"))\n",
    "\n",
    "    model.compile(\n",
    "        optimizer=\"adam\", loss=\"categorical_crossentropy\", metrics=[\"accuracy\"]\n",
    "    )\n",
    "    return model\n",
    "\n",
    "\n",
    "# Initialize the Keras Tuner\n",
    "tuner = RandomSearch(\n",
    "    build_model,\n",
    "    objective=\"val_accuracy\",\n",
    "    max_trials=10,\n",
    "    executions_per_trial=2,\n",
    "    directory=\"my_dir\",\n",
    "    project_name=\"hyperparameter_tuning\",\n",
    ")\n",
    "\n",
    "# Perform hyperparameter tuning\n",
    "tuner.search(X_train, y_train_encoded, epochs=50, validation_split=0.2, verbose=2)\n",
    "\n",
    "# Get the optimal hyperparameters\n",
    "best_hps = tuner.get_best_hyperparameters(num_trials=1)[0]\n",
    "print(f\"Best Hyperparameters: {best_hps.values}\")\n",
    "\n",
    "# Train the model with the best hyperparameters\n",
    "best_model = tuner.hypermodel.build(best_hps)\n",
    "history_best = best_model.fit(\n",
    "    X_train, y_train_encoded, epochs=100, batch_size=10, validation_split=0.2, verbose=2\n",
    ")\n",
    "\n",
    "# Evaluate the model\n",
    "loss_best, accuracy_best = best_model.evaluate(X_test, y_test_encoded)\n",
    "print(f\"Test Accuracy with Best Hyperparameters: {accuracy_best}\")\n",
    "\n",
    "# Make predictions\n",
    "y_pred_best_encoded = best_model.predict(X_test)\n",
    "y_pred_best = encoder.inverse_transform(y_pred_best_encoded)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy_best = accuracy_score(y_test, y_pred_best)\n",
    "print(f\"Accuracy with Best Hyperparameters: {accuracy_best}\")\n",
    "print(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred_best))\n",
    "print(\"\\nConfusion Matrix:\\n\", confusion_matrix(y_test, y_pred_best))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Notes on Expanding Neural Networks Analysis\n",
    "\n",
    "1. **Different Neural Network Architectures**:\n",
    "\n",
    "   - Explore different neural network architectures, such as deeper networks, convolutional neural networks (CNNs) for image data, and recurrent neural networks (RNNs) for sequence data.\n",
    "   - Compare their performance on various tasks.\n",
    "\n",
    "2. **Regularization Techniques**:\n",
    "\n",
    "   - Implement regularization techniques such as dropout, L1/L2 regularization, and batch normalization to prevent overfitting.\n",
    "   - Evaluate the impact of these techniques on the model performance.\n",
    "\n",
    "3. **Advanced Optimizers**:\n",
    "\n",
    "   - Experiment with advanced optimization algorithms such as RMSprop, Adam, and Nadam.\n",
    "   - Compare their performance with the standard stochastic gradient descent (SGD).\n",
    "\n",
    "4. **Transfer Learning**:\n",
    "\n",
    "   - Utilize pre-trained models for transfer learning in tasks like image classification and natural language processing.\n",
    "   - Fine-tune the pre-trained models on your specific dataset.\n",
    "\n",
    "5. **Real-world Applications**:\n",
    "\n",
    "   - Apply neural network models to real-world datasets, such as image classification, text classification, and time-series forecasting.\n",
    "   - Discuss practical challenges and considerations when implementing neural networks in real-world scenarios.\n",
    "\n",
    "6. **Model Interpretability**:\n",
    "\n",
    "   - Use techniques like SHAP (SHapley Additive exPlanations) and LIME (Local Interpretable Model-agnostic Explanations) to interpret neural network predictions.\n",
    "   - Visualize the contributions of different features to the model's predictions.\n",
    "\n",
    "7. **Scalability and Efficiency**:\n",
    "   - Discuss techniques to improve the scalability and efficiency of neural networks, such as distributed training and model parallelism.\n",
    "   - Implement neural networks using frameworks like TensorFlow, PyTorch, and Keras.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13. Case Studies\n",
    "\n",
    "### 13.1 Real-world Datasets Analysis\n",
    "\n",
    "In this section, we will perform an end-to-end statistical analysis on a real-world dataset. We will go through the steps of data preprocessing, exploratory data analysis (EDA), model building, evaluation, and interpretation of results.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 13.1.1 Problem Definition and Data Loading\n",
    "\n",
    "For this case study, we will use the \"Heart Disease\" dataset from the UCI Machine Learning Repository. The goal is to predict whether a patient has heart disease based on various features.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Problem Definition and Data Loading\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Load the dataset\n",
    "url = \"https://archive.ics.uci.edu/ml/machine-learning-databases/heart-disease/processed.cleveland.data\"\n",
    "columns = [\n",
    "    \"age\",\n",
    "    \"sex\",\n",
    "    \"cp\",\n",
    "    \"trestbps\",\n",
    "    \"chol\",\n",
    "    \"fbs\",\n",
    "    \"restecg\",\n",
    "    \"thalach\",\n",
    "    \"exang\",\n",
    "    \"oldpeak\",\n",
    "    \"slope\",\n",
    "    \"ca\",\n",
    "    \"thal\",\n",
    "    \"target\",\n",
    "]\n",
    "heart_data = pd.read_csv(url, names=columns)\n",
    "\n",
    "# Display the first few rows of the dataset\n",
    "print(\"Heart Disease Dataset:\")\n",
    "print(heart_data.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 13.1.2 Data Preprocessing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Preprocessing\n",
    "\n",
    "# Replace missing values marked with '?' with NaN\n",
    "heart_data.replace(\"?\", np.nan, inplace=True)\n",
    "\n",
    "# Convert columns with numeric data stored as strings to float\n",
    "heart_data = heart_data.astype(float)\n",
    "\n",
    "# Handle missing values (for simplicity, we'll drop rows with missing values)\n",
    "heart_data.dropna(inplace=True)\n",
    "\n",
    "# Split the data into features and target variable\n",
    "X = heart_data.drop(columns=[\"target\"])\n",
    "y = heart_data[\"target\"]\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.3, random_state=42\n",
    ")\n",
    "\n",
    "# Standardize the data\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# Display the shape of the data\n",
    "print(f\"Training data shape: {X_train.shape}\")\n",
    "print(f\"Testing data shape: {X_test.shape}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 13.1.3 Exploratory Data Analysis (EDA)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exploratory Data Analysis (EDA)\n",
    "\n",
    "# Plot the distribution of the target variable\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.countplot(x=\"target\", data=heart_data)\n",
    "plt.title(\"Distribution of Target Variable\")\n",
    "plt.xlabel(\"Heart Disease\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.show()\n",
    "\n",
    "# Plot the correlation matrix\n",
    "plt.figure(figsize=(12, 10))\n",
    "correlation_matrix = heart_data.corr()\n",
    "sns.heatmap(correlation_matrix, annot=True, cmap=\"coolwarm\")\n",
    "plt.title(\"Correlation Matrix\")\n",
    "plt.show()\n",
    "\n",
    "# Plot the distribution of numerical features\n",
    "heart_data.hist(bins=20, figsize=(14, 10))\n",
    "plt.suptitle(\"Distribution of Numerical Features\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 13.1.4 Model Building and Evaluation\n",
    "\n",
    "#### Logistic Regression\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Building and Evaluation - Logistic Regression\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Initialize and train the logistic regression model\n",
    "log_reg = LogisticRegression(random_state=42)\n",
    "log_reg.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred_log_reg = log_reg.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy_log_reg = accuracy_score(y_test, y_pred_log_reg)\n",
    "print(f\"Logistic Regression Accuracy: {accuracy_log_reg}\")\n",
    "print(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred_log_reg))\n",
    "print(\"\\nConfusion Matrix:\\n\", confusion_matrix(y_test, y_pred_log_reg))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Random Forest Classifier\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Building and Evaluation - Random Forest Classifier\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Initialize and train the random forest classifier\n",
    "rf_clf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "rf_clf.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred_rf = rf_clf.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy_rf = accuracy_score(y_test, y_pred_rf)\n",
    "print(f\"Random Forest Classifier Accuracy: {accuracy_rf}\")\n",
    "print(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred_rf))\n",
    "print(\"\\nConfusion Matrix:\\n\", confusion_matrix(y_test, y_pred_rf))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 13.1.5 Interpretation of Results\n",
    "\n",
    "Compare the performance of the Logistic Regression and Random Forest Classifier models based on the accuracy, classification report, and confusion matrix. Discuss which model performs better for this dataset and why.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Interpretation of Results\n",
    "\n",
    "# Logistic Regression Results\n",
    "print(\"Logistic Regression Results:\")\n",
    "print(f\"Accuracy: {accuracy_log_reg}\")\n",
    "print(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred_log_reg))\n",
    "print(\"\\nConfusion Matrix:\\n\", confusion_matrix(y_test, y_pred_log_reg))\n",
    "\n",
    "# Random Forest Classifier Results\n",
    "print(\"Random Forest Classifier Results:\")\n",
    "print(f\"Accuracy: {accuracy_rf}\")\n",
    "print(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred_rf))\n",
    "print(\"\\nConfusion Matrix:\\n\", confusion_matrix(y_test, y_pred_rf))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Notes on Expanding Real-world Datasets Analysis\n",
    "\n",
    "1. **Feature Engineering**:\n",
    "\n",
    "   - Create new features from existing ones, such as interaction terms or polynomial features.\n",
    "   - Evaluate the impact of feature engineering on model performance.\n",
    "\n",
    "2. **Advanced Preprocessing Techniques**:\n",
    "\n",
    "   - Handle missing values using imputation methods.\n",
    "   - Address class imbalance using techniques such as SMOTE (Synthetic Minority Over-sampling Technique).\n",
    "\n",
    "3. **Comparison with Other Models**:\n",
    "\n",
    "   - Compare the performance of different models, such as support vector machines (SVM), gradient boosting, and neural networks.\n",
    "   - Discuss the advantages and disadvantages of each model.\n",
    "\n",
    "4. **Hyperparameter Tuning**:\n",
    "\n",
    "   - Perform hyperparameter tuning for each model to optimize performance.\n",
    "   - Compare the results with the default parameters.\n",
    "\n",
    "5. **Cross-validation**:\n",
    "\n",
    "   - Implement cross-validation to ensure robust model evaluation.\n",
    "   - Compare the performance of models using cross-validation scores.\n",
    "\n",
    "6. **Interpretability and Explainability**:\n",
    "\n",
    "   - Use model interpretability techniques such as SHAP (SHapley Additive exPlanations) and LIME (Local Interpretable Model-agnostic Explanations) to understand the model's predictions.\n",
    "   - Visualize the feature importance and the impact of each feature on the predictions.\n",
    "\n",
    "7. **Deployment and Monitoring**:\n",
    "   - Discuss the steps to deploy the best-performing model in a production environment.\n",
    "   - Implement monitoring to track the model's performance over time and detect any degradation.\n",
    "\n",
    "By expanding on these topics, you can gain a deeper understanding of end-to-end statistical analysis, from data preprocessing and exploratory data analysis to model building, evaluation, and interpretation. This comprehensive approach ensures that you are well-prepared to handle real-world datasets and challenges.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13.2 End-to-End Statistical Analysis Project\n",
    "\n",
    "In this section, we will go through a complete end-to-end statistical analysis project using a real-world dataset. We will define the problem, preprocess the data, perform exploratory data analysis (EDA), build and evaluate models, and interpret the results.\n",
    "\n",
    "### 13.2.1 Problem Definition and Data Loading\n",
    "\n",
    "The first step in any data analysis project is to clearly define the problem you are trying to solve. For this project, we will use the \"Titanic\" dataset to predict whether a passenger survived the Titanic disaster based on various features.\n",
    "\n",
    "#### Problem Definition:\n",
    "\n",
    "- **Objective**: Predict the survival of passengers on the Titanic.\n",
    "- **Target Variable**: `Survived` (0 = No, 1 = Yes)\n",
    "- **Features**: Various passenger attributes such as age, sex, class, fare, etc.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Notes on Problem Definition and Data Loading\n",
    "\n",
    "1. **Understand the Dataset**:\n",
    "\n",
    "   - Read the dataset documentation to understand the context and meaning of each feature.\n",
    "   - Identify the target variable and the features.\n",
    "\n",
    "2. **Initial Data Exploration**:\n",
    "\n",
    "   - Display the first few rows of the dataset to get an initial sense of the data.\n",
    "   - Check for missing values and data types to plan the preprocessing steps.\n",
    "\n",
    "3. **Setting Up the Environment**:\n",
    "   - Ensure you have all the necessary libraries and packages installed (e.g., pandas, numpy, scikit-learn, matplotlib, seaborn).\n",
    "\n",
    "By clearly defining the problem and loading the dataset, you set the foundation for the subsequent steps in the analysis project. Understanding the dataset and its structure is crucial for effective data preprocessing and modeling.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 13.2.2 Data Preprocessing\n",
    "\n",
    "Data preprocessing involves cleaning and transforming the raw data into a suitable format for analysis and modeling. This includes handling missing values, encoding categorical variables, scaling numerical features, and splitting the data into training and testing sets.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Handle missing values\n",
    "titanic_data[\"Age\"].fillna(titanic_data[\"Age\"].median(), inplace=True)\n",
    "titanic_data[\"Embarked\"].fillna(titanic_data[\"Embarked\"].mode()[0], inplace=True)\n",
    "titanic_data.drop(\n",
    "    columns=[\"Cabin\"], inplace=True\n",
    ")  # Drop the 'Cabin' column due to too many missing values\n",
    "\n",
    "# Convert categorical variables to numeric using one-hot encoding\n",
    "titanic_data = pd.get_dummies(\n",
    "    titanic_data, columns=[\"Sex\", \"Embarked\", \"Pclass\"], drop_first=True\n",
    ")\n",
    "\n",
    "# Drop irrelevant columns\n",
    "titanic_data.drop(columns=[\"Name\", \"Ticket\", \"PassengerId\"], inplace=True)\n",
    "\n",
    "# Split the data into features and target variable\n",
    "X = titanic_data.drop(columns=[\"Survived\"])\n",
    "y = titanic_data[\"Survived\"]\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.3, random_state=42\n",
    ")\n",
    "\n",
    "# Standardize the numerical features\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# Display the shape of the data\n",
    "print(f\"Training data shape: {X_train.shape}\")\n",
    "print(f\"Testing data shape: {X_test.shape}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Notes on Data Preprocessing\n",
    "\n",
    "1. **Handling Missing Values**:\n",
    "\n",
    "   - Impute missing values using appropriate strategies (e.g., mean, median, mode, or more advanced techniques).\n",
    "   - Drop columns with too many missing values if they cannot be reasonably imputed.\n",
    "\n",
    "2. **Encoding Categorical Variables**:\n",
    "\n",
    "   - Convert categorical variables to numeric using techniques like one-hot encoding, label encoding, or target encoding.\n",
    "\n",
    "3. **Feature Scaling**:\n",
    "\n",
    "   - Standardize or normalize numerical features to ensure they are on a similar scale, which can improve model performance.\n",
    "\n",
    "4. **Splitting the Data**:\n",
    "   - Split the data into training and testing sets to evaluate the model's performance on unseen data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 13.2.3 Exploratory Data Analysis (EDA)\n",
    "\n",
    "Exploratory Data Analysis (EDA) involves visualizing and summarizing the data to understand its distribution, relationships, and potential issues. This step helps to generate insights and inform subsequent modeling decisions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the distribution of the target variable\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.countplot(x=\"Survived\", data=titanic_data)\n",
    "plt.title(\"Distribution of Target Variable\")\n",
    "plt.xlabel(\"Survived\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.show()\n",
    "\n",
    "# Plot the distribution of numerical features\n",
    "titanic_data.hist(bins=20, figsize=(14, 10))\n",
    "plt.suptitle(\"Distribution of Numerical Features\")\n",
    "plt.show()\n",
    "\n",
    "# Plot the correlation matrix\n",
    "plt.figure(figsize=(12, 10))\n",
    "correlation_matrix = titanic_data.corr()\n",
    "sns.heatmap(correlation_matrix, annot=True, cmap=\"coolwarm\")\n",
    "plt.title(\"Correlation Matrix\")\n",
    "plt.show()\n",
    "\n",
    "# Box plot for age distribution by survival status\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.boxplot(x=\"Survived\", y=\"Age\", data=titanic_data)\n",
    "plt.title(\"Age Distribution by Survival Status\")\n",
    "plt.xlabel(\"Survived\")\n",
    "plt.ylabel(\"Age\")\n",
    "plt.show()\n",
    "\n",
    "# Count plot for survival status by sex\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.countplot(x=\"Survived\", hue=\"Sex_male\", data=titanic_data)\n",
    "plt.title(\"Survival Status by Sex\")\n",
    "plt.xlabel(\"Survived\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Notes on Exploratory Data Analysis (EDA)\n",
    "\n",
    "1. **Visualizing the Target Variable**:\n",
    "\n",
    "   - Plot the distribution of the target variable to understand the class balance.\n",
    "\n",
    "2. **Visualizing Numerical Features**:\n",
    "\n",
    "   - Use histograms, box plots, and scatter plots to understand the distribution and relationships of numerical features.\n",
    "\n",
    "3. **Visualizing Categorical Features**:\n",
    "\n",
    "   - Use count plots and bar plots to visualize the distribution and relationships of categorical features.\n",
    "\n",
    "4. **Correlation Analysis**:\n",
    "\n",
    "   - Plot the correlation matrix to identify potential relationships between features and the target variable.\n",
    "\n",
    "5. **Identifying Patterns and Anomalies**:\n",
    "   - Look for patterns, trends, and anomalies in the data that can inform feature engineering and modeling decisions.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 13.2.4 Model Building and Evaluation\n",
    "\n",
    "In this step, we will build and evaluate several machine learning models to predict whether a passenger survived the Titanic disaster. We will use Logistic Regression and Random Forest Classifier as examples.\n",
    "\n",
    "#### Logistic Regression\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Building and Evaluation - Logistic Regression\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Initialize and train the logistic regression model\n",
    "log_reg = LogisticRegression(random_state=42)\n",
    "log_reg.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred_log_reg = log_reg.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy_log_reg = accuracy_score(y_test, y_pred_log_reg)\n",
    "print(f\"Logistic Regression Accuracy: {accuracy_log_reg}\")\n",
    "print(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred_log_reg))\n",
    "print(\"\\nConfusion Matrix:\\n\", confusion_matrix(y_test, y_pred_log_reg))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Random Forest Classifier\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Building and Evaluation - Random Forest Classifier\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Initialize and train the random forest classifier\n",
    "rf_clf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "rf_clf.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred_rf = rf_clf.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy_rf = accuracy_score(y_test, y_pred_rf)\n",
    "print(f\"Random Forest Classifier Accuracy: {accuracy_rf}\")\n",
    "print(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred_rf))\n",
    "print(\"\\nConfusion Matrix:\\n\", confusion_matrix(y_test, y_pred_rf))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Additional Models (Optional)\n",
    "\n",
    "You can experiment with other models such as Support Vector Machines (SVM), Gradient Boosting, or Neural Networks to see if they provide better performance.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Support Vector Machine (SVM)\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "# Initialize and train the SVM model\n",
    "svm_clf = SVC(random_state=42)\n",
    "svm_clf.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred_svm = svm_clf.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy_svm = accuracy_score(y_test, y_pred_svm)\n",
    "print(f\"SVM Accuracy: {accuracy_svm}\")\n",
    "print(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred_svm))\n",
    "print(\"\\nConfusion Matrix:\\n\", confusion_matrix(y_test, y_pred_svm))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 13.2.5 Interpretation of Results\n",
    "\n",
    "In this step, we will interpret the results of the models built in the previous step. We will compare the performance of different models based on accuracy, classification report, and confusion matrix. We will also discuss the strengths and weaknesses of each model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Interpretation of Results\n",
    "\n",
    "# Logistic Regression Results\n",
    "print(\"Logistic Regression Results:\")\n",
    "print(f\"Accuracy: {accuracy_log_reg}\")\n",
    "print(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred_log_reg))\n",
    "print(\"\\nConfusion Matrix:\\n\", confusion_matrix(y_test, y_pred_log_reg))\n",
    "\n",
    "# Random Forest Classifier Results\n",
    "print(\"Random Forest Classifier Results:\")\n",
    "print(f\"Accuracy: {accuracy_rf}\")\n",
    "print(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred_rf))\n",
    "print(\"\\nConfusion Matrix:\\n\", confusion_matrix(y_test, y_pred_rf))\n",
    "\n",
    "# Optional: SVM Results\n",
    "print(\"SVM Results:\")\n",
    "print(f\"Accuracy: {accuracy_svm}\")\n",
    "print(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred_svm))\n",
    "print(\"\\nConfusion Matrix:\\n\", confusion_matrix(y_test, y_pred_svm))\n",
    "\n",
    "# Compare the models\n",
    "print(\"\\nModel Comparison:\")\n",
    "print(f\"Logistic Regression Accuracy: {accuracy_log_reg}\")\n",
    "print(f\"Random Forest Classifier Accuracy: {accuracy_rf}\")\n",
    "print(f\"SVM Accuracy: {accuracy_svm}\")\n",
    "\n",
    "# Discuss strengths and weaknesses\n",
    "print(\"\\nStrengths and Weaknesses:\")\n",
    "print(\"Logistic Regression:\")\n",
    "print(\"- Strengths: Simple, interpretable, fast to train\")\n",
    "print(\n",
    "    \"- Weaknesses: Assumes linear relationship, may underperform with complex relationships\"\n",
    ")\n",
    "print(\"\\nRandom Forest Classifier:\")\n",
    "print(\n",
    "    \"- Strengths: Handles complex relationships, less prone to overfitting, feature importance\"\n",
    ")\n",
    "print(\"- Weaknesses: Less interpretable, more computationally intensive\")\n",
    "print(\"\\nSVM:\")\n",
    "print(\"- Strengths: Effective in high-dimensional spaces, flexible with kernel trick\")\n",
    "print(\"- Weaknesses: Computationally intensive, harder to tune hyperparameters\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Notes on Interpretation of Results\n",
    "\n",
    "1. **Model Comparison**:\n",
    "\n",
    "   - Compare the performance of different models based on accuracy, precision, recall, F1-score, and confusion matrix.\n",
    "   - Discuss which model performed best and why.\n",
    "\n",
    "2. **Strengths and Weaknesses**:\n",
    "\n",
    "   - Identify the strengths and weaknesses of each model.\n",
    "   - Discuss scenarios where each model would be preferable.\n",
    "\n",
    "3. **Model Interpretation**:\n",
    "   - Use model interpretability techniques (e.g., feature importance for Random Forest, coefficients for Logistic Regression) to understand the key features driving the predictions.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 14. Conclusion\n",
    "\n",
    "In this notebook, we have covered a wide range of advanced statistical analysis techniques using Python. Below is a summary of the key takeaways, resources for further reading, and next steps for continued learning and application.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 14.1 Summary of Key Takeaways\n",
    "\n",
    "### Summary of Key Takeaways\n",
    "\n",
    "1. **Descriptive Statistics**:\n",
    "\n",
    "   - Techniques to summarize and describe the main features of a dataset.\n",
    "\n",
    "2. **Inferential Statistics**:\n",
    "\n",
    "   - Methods to make inferences about a population based on sample data, including hypothesis testing and confidence intervals.\n",
    "\n",
    "3. **Regression Analysis**:\n",
    "\n",
    "   - Simple and multiple linear regression, polynomial regression, and model evaluation metrics.\n",
    "\n",
    "4. **Advanced Regression Techniques**:\n",
    "\n",
    "   - Ridge regression, Lasso regression, Elastic Net, and Logistic regression.\n",
    "\n",
    "5. **Time Series Analysis**:\n",
    "\n",
    "   - Decomposition of time series, autocorrelation and partial autocorrelation functions, ARIMA and SARIMA models.\n",
    "\n",
    "6. **Principal Component Analysis (PCA)**:\n",
    "\n",
    "   - Dimensionality reduction techniques and interpretation of principal components.\n",
    "\n",
    "7. **Clustering**:\n",
    "\n",
    "   - K-means clustering, hierarchical clustering, DBSCAN, and Gaussian Mixture Models (GMM).\n",
    "\n",
    "8. **Advanced Statistical Models**:\n",
    "\n",
    "   - Generalized Linear Models (GLMs), survival analysis, and Bayesian statistics.\n",
    "\n",
    "9. **Resampling Methods**:\n",
    "\n",
    "   - Bootstrap and cross-validation techniques.\n",
    "\n",
    "10. **Statistical Machine Learning**:\n",
    "\n",
    "    - Decision trees, random forests, support vector machines (SVM), and neural networks.\n",
    "\n",
    "11. **Case Studies**:\n",
    "    - End-to-end analysis of real-world datasets, including problem definition, data preprocessing, exploratory data analysis, model building, evaluation, and interpretation of results.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 14.2 Further Reading and Resources\n",
    "\n",
    "### Further Reading and Resources\n",
    "\n",
    "1. **Books**:\n",
    "\n",
    "   - \"Introduction to Statistical Learning\" by Gareth James, Daniela Witten, Trevor Hastie, and Robert Tibshirani.\n",
    "   - \"Applied Predictive Modeling\" by Max Kuhn and Kjell Johnson.\n",
    "   - \"Pattern Recognition and Machine Learning\" by Christopher M. Bishop.\n",
    "\n",
    "2. **Online Courses**:\n",
    "\n",
    "   - \"Machine Learning\" by Andrew Ng on Coursera.\n",
    "   - \"Data Science and Machine Learning Bootcamp with R\" by Jose Portilla on Udemy.\n",
    "   - \"Deep Learning Specialization\" by Andrew Ng on Coursera.\n",
    "\n",
    "3. **Documentation and Tutorials**:\n",
    "\n",
    "   - [Scikit-learn Documentation](https://scikit-learn.org/stable/documentation.html)\n",
    "   - [TensorFlow Documentation](https://www.tensorflow.org/learn)\n",
    "   - [PyMC3 Documentation](https://docs.pymc.io/)\n",
    "\n",
    "4. **Research Papers**:\n",
    "\n",
    "   - \"A Few Useful Things to Know About Machine Learning\" by Pedro Domingos.\n",
    "   - \"The Elements of Statistical Learning\" by Trevor Hastie, Robert Tibshirani, and Jerome Friedman.\n",
    "\n",
    "5. **Blogs and Articles**:\n",
    "   - Towards Data Science Blog on Medium.\n",
    "   - Analytics Vidhya.\n",
    "   - DataCamp Blog.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 14.3 Next Steps for Learning and Application\n",
    "\n",
    "### Next Steps for Learning and Application\n",
    "\n",
    "1. **Practical Implementation**:\n",
    "\n",
    "   - Apply the techniques learned to new datasets and real-world problems.\n",
    "   - Participate in data science competitions on platforms like Kaggle to practice and improve your skills.\n",
    "\n",
    "2. **Advanced Topics**:\n",
    "\n",
    "   - Explore advanced topics such as deep learning, reinforcement learning, and natural language processing.\n",
    "   - Delve deeper into Bayesian statistics and hierarchical modeling.\n",
    "\n",
    "3. **Collaboration and Networking**:\n",
    "\n",
    "   - Join data science and machine learning communities, attend conferences, and participate in meetups.\n",
    "   - Collaborate with others on projects to gain diverse perspectives and experiences.\n",
    "\n",
    "4. **Continuous Learning**:\n",
    "\n",
    "   - Stay updated with the latest research and advancements in the field.\n",
    "   - Take advanced courses and read recent publications to expand your knowledge.\n",
    "\n",
    "5. **Building a Portfolio**:\n",
    "   - Create and maintain a portfolio of your projects and analyses.\n",
    "   - Share your work on GitHub, personal blogs, and professional networks like LinkedIn.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 15. References and Resources\n",
    "\n",
    "This section provides a curated list of books, papers, articles, libraries, tools, online courses, and tutorials to further your learning and understanding of advanced statistical analysis and machine learning using Python.\n",
    "\n",
    "### 15.1 Books, Papers, and Articles for Further Reading\n",
    "\n",
    "1. **Books**:\n",
    "\n",
    "   - \"Introduction to Statistical Learning\" by Gareth James, Daniela Witten, Trevor Hastie, and Robert Tibshirani.\n",
    "   - \"The Elements of Statistical Learning\" by Trevor Hastie, Robert Tibshirani, and Jerome Friedman.\n",
    "   - \"Applied Predictive Modeling\" by Max Kuhn and Kjell Johnson.\n",
    "   - \"Pattern Recognition and Machine Learning\" by Christopher M. Bishop.\n",
    "   - \"Bayesian Data Analysis\" by Andrew Gelman, John B. Carlin, Hal S. Stern, David B. Dunson, Aki Vehtari, and Donald B. Rubin.\n",
    "   - \"Deep Learning\" by Ian Goodfellow, Yoshua Bengio, and Aaron Courville.\n",
    "\n",
    "2. **Papers**:\n",
    "\n",
    "   - \"A Few Useful Things to Know About Machine Learning\" by Pedro Domingos.\n",
    "   - \"The Nature of Statistical Learning Theory\" by Vladimir Vapnik.\n",
    "   - \"Random Forests\" by Leo Breiman.\n",
    "   - \"Stochastic Gradient Descent Tricks\" by Lon Bottou.\n",
    "\n",
    "3. **Articles and Blogs**:\n",
    "   - \"Understanding Random Forests\" by Edwin Chen on Medium.\n",
    "   - \"Introduction to Statistical Learning with Applications in R\" by James et al. (available as a free PDF online).\n",
    "   - \"A Beginners Guide to Neural Networks and Deep Learning\" by Pathmind.\n",
    "   - \"How to Choose an Activation Function for Deep Learning\" by Jason Brownlee on Machine Learning Mastery.\n",
    "\n",
    "### 15.2 Useful Libraries and Tools\n",
    "\n",
    "1. **Scikit-learn**:\n",
    "\n",
    "   - A powerful library for machine learning in Python that provides simple and efficient tools for data analysis and modeling.\n",
    "   - [Scikit-learn Documentation](https://scikit-learn.org/stable/documentation.html)\n",
    "\n",
    "2. **TensorFlow and Keras**:\n",
    "\n",
    "   - TensorFlow is an open-source library for numerical computation and machine learning.\n",
    "   - Keras is a high-level neural networks API that runs on top of TensorFlow.\n",
    "   - [TensorFlow Documentation](https://www.tensorflow.org/learn)\n",
    "   - [Keras Documentation](https://keras.io/)\n",
    "\n",
    "3. **PyTorch**:\n",
    "\n",
    "   - An open-source machine learning library developed by Facebooks AI Research lab, known for its flexibility and dynamic computation graph.\n",
    "   - [PyTorch Documentation](https://pytorch.org/docs/stable/index.html)\n",
    "\n",
    "4. **Statsmodels**:\n",
    "\n",
    "   - A library for estimating and testing statistical models in Python.\n",
    "   - [Statsmodels Documentation](https://www.statsmodels.org/stable/index.html)\n",
    "\n",
    "5. **PyMC3**:\n",
    "\n",
    "   - A Python library for probabilistic programming that allows for Bayesian statistical modeling and inference.\n",
    "   - [PyMC3 Documentation](https://docs.pymc.io/)\n",
    "\n",
    "6. **Seaborn**:\n",
    "\n",
    "   - A statistical data visualization library based on Matplotlib that provides a high-level interface for drawing attractive and informative graphics.\n",
    "   - [Seaborn Documentation](https://seaborn.pydata.org/)\n",
    "\n",
    "7. **Matplotlib**:\n",
    "\n",
    "   - A comprehensive library for creating static, animated, and interactive visualizations in Python.\n",
    "   - [Matplotlib Documentation](https://matplotlib.org/stable/contents.html)\n",
    "\n",
    "8. **Pandas**:\n",
    "   - A powerful data manipulation and analysis library for Python.\n",
    "   - [Pandas Documentation](https://pandas.pydata.org/pandas-docs/stable/)\n",
    "\n",
    "### 15.3 Online Courses and Tutorials\n",
    "\n",
    "1. **Machine Learning by Andrew Ng on Coursera**:\n",
    "\n",
    "   - A comprehensive course that covers a broad range of machine learning topics, including supervised learning, unsupervised learning, and neural networks.\n",
    "   - [Machine Learning Course](https://www.coursera.org/learn/machine-learning)\n",
    "\n",
    "2. **Deep Learning Specialization by Andrew Ng on Coursera**:\n",
    "\n",
    "   - A series of courses that provide a deep dive into deep learning, covering neural networks, CNNs, RNNs, and more.\n",
    "   - [Deep Learning Specialization](https://www.coursera.org/specializations/deep-learning)\n",
    "\n",
    "3. **Data Science and Machine Learning Bootcamp with R by Jose Portilla on Udemy**:\n",
    "\n",
    "   - A bootcamp-style course that covers data science and machine learning concepts and applications using R.\n",
    "   - [Data Science and Machine Learning Bootcamp](https://www.udemy.com/course/python-for-data-science-and-machine-learning-bootcamp/)\n",
    "\n",
    "4. **Python for Data Science and Machine Learning Bootcamp by Jose Portilla on Udemy**:\n",
    "\n",
    "   - A comprehensive course that teaches data science and machine learning using Python, including popular libraries like Pandas, Seaborn, and Scikit-learn.\n",
    "   - [Python for Data Science and Machine Learning Bootcamp](https://www.udemy.com/course/python-for-data-science-and-machine-learning-bootcamp/)\n",
    "\n",
    "5. **MIT OpenCourseWare - Introduction to Computer Science and Programming in Python**:\n",
    "\n",
    "   - An introductory course that teaches basic programming concepts using Python.\n",
    "   - [MIT OCW - Introduction to Computer Science and Programming](https://ocw.mit.edu/courses/electrical-engineering-and-computer-science/6-0001-introduction-to-computer-science-and-programming-in-python-fall-2016/)\n",
    "\n",
    "6. **DataCamp**:\n",
    "\n",
    "   - An online platform that offers interactive courses on data science, machine learning, and programming.\n",
    "   - [DataCamp](https://www.datacamp.com/)\n",
    "\n",
    "7. **Kaggle**:\n",
    "   - A platform for data science competitions that also provides a wealth of datasets, kernels (notebooks), and learning resources.\n",
    "   - [Kaggle](https://www.kaggle.com/learn)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
