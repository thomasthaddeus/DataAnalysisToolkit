{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Additional Exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Exercise 1: Integrating Multiple Data Sources\n",
    "- **Objective**: Combine data from an Excel file, a SQL database, and a JSON API into a single DataFrame.\n",
    "- **Tasks**:\n",
    "  - Use the Excel Connector to load data from an Excel file.\n",
    "  - Fetch data from a SQL database using the SQL Connector.\n",
    "  - Retrieve data from a JSON API using the API Connector.\n",
    "  - Integrate all these datasets using the Data Integrator.\n",
    "- **Challenge**: Ensure that the integrated dataset is properly aligned and handle any inconsistencies in data formats or missing values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 1: Use the Excel Connector to load data from an Excel file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataanalysistoolkit.data_sources import ExcelConnector, SQLConnector, APIConnector\n",
    "from dataanalysistoolkit.integrators import DataIntegrator\n",
    "import pandas as pd\n",
    "\n",
    "# Load data from the provided Excel file\n",
    "excel_connector = ExcelConnector('/mnt/data/example_data.xlsx')\n",
    "df_excel = excel_connector.load_data(sheet_name='Sheet1')\n",
    "print(\"Data from Excel:\")\n",
    "print(df_excel.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 2: Fetch data from a SQL database using the SQL Connector.\n",
    "\n",
    "For this step, we'll assume you have a PostgreSQL database set up with a table named `customer_data`. If you need to adjust the connection details or table name, you can do so in the code below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace with your actual database URI\n",
    "sql_connector = SQLConnector('postgresql://username:password@localhost:5432/mydatabase')\n",
    "\n",
    "# Executing a SQL query to fetch data\n",
    "query = \"SELECT * FROM customer_data LIMIT 5\"\n",
    "df_sql = sql_connector.query_data(query)\n",
    "print(\"Data from SQL Database:\")\n",
    "print(df_sql.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 3: Retrieve data from a JSON API using the API Connector.\n",
    "\n",
    "For demonstration purposes, we'll assume you have access to an API endpoint. You can replace the URL and authentication details as needed.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace with the actual API base URL and authentication credentials\n",
    "api_connector = APIConnector('https://api.example.com', auth=('username', 'password'))\n",
    "\n",
    "# Fetching data from a specific endpoint\n",
    "endpoint = 'data_endpoint'\n",
    "response = api_connector.get(endpoint)\n",
    "\n",
    "# Assuming the response is JSON and converting it to a DataFrame\n",
    "df_api = pd.DataFrame(response.json())\n",
    "print(\"Data from API:\")\n",
    "print(df_api.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 4: Integrate all these datasets using the Data Integrator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the Data Integrator\n",
    "integrator = DataIntegrator()\n",
    "\n",
    "# Adding dataframes to the integrator\n",
    "integrator.add_data(df_excel)\n",
    "integrator.add_data(df_sql)\n",
    "integrator.add_data(df_api)\n",
    "\n",
    "# Assuming we want to concatenate the dataframes\n",
    "combined_df = integrator.concatenate_data()\n",
    "print(\"Combined Data:\")\n",
    "print(combined_df.head())\n",
    "\n",
    "# Alternatively, if you need to merge the dataframes on a common key:\n",
    "# combined_df = integrator.merge_data(on='common_key')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Challenge\n",
    "\n",
    "Ensure that the integrated dataset is properly aligned and handle any inconsistencies in data formats or missing values.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataanalysistoolkit.preprocessor import DataFormatter\n",
    "\n",
    "# Initialize the Data Formatter\n",
    "formatter = DataFormatter(combined_df)\n",
    "\n",
    "# Standardize date formats\n",
    "formatter.standardize_dates('date_column', date_format='%Y-%m-%d')\n",
    "\n",
    "# Normalize numeric columns\n",
    "numeric_columns = ['numeric_column1', 'numeric_column2']\n",
    "formatter.normalize_numeric(numeric_columns)\n",
    "\n",
    "# Categorize columns\n",
    "category_columns = ['category_column1', 'category_column2']\n",
    "formatter.categorize_columns(category_columns)\n",
    "\n",
    "# Fill missing values\n",
    "formatter.fill_missing_values('column_with_missing_data', fill_value=0)\n",
    "\n",
    "print(\"Cleaned and Formatted Combined Data:\")\n",
    "print(combined_df.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This completes Exercise 1 in the Jupyter notebook. You have successfully integrated data from an Excel file, a SQL database, and a JSON API into a single DataFrame, then cleaned and formatted the data for further analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Exercise 2: Data Cleaning and Transformation\n",
    "- **Objective**: Clean and transform the integrated dataset from Exercise 1.\n",
    "- **Tasks**:\n",
    "  - Identify and fill missing values in the dataset.\n",
    "  - Standardize the format of any date columns.\n",
    "  - Normalize numeric columns and convert categorical columns to a standard format.\n",
    "  - Create a new column based on a custom transformation logic.\n",
    "- **Challenge**: Try to automate as much of the data cleaning process as possible, considering future data imports."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll assume that `combined_df` is the DataFrame we obtained from Exercise 1.\n",
    "\n",
    "### Exercise 2: Data Cleaning and Transformation\n",
    "\n",
    "**Objective:** Clean and transform the integrated dataset from Exercise 1.\n",
    "\n",
    "#### Step 1: Identify and fill missing values in the dataset.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking for missing values in the combined dataframe\n",
    "print(\"Missing values before filling:\")\n",
    "print(combined_df.isnull().sum())\n",
    "\n",
    "# Fill missing values in the 'column_with_missing_data' column with a specified value\n",
    "formatter.fill_missing_values('column_with_missing_data', fill_value=0)\n",
    "\n",
    "# Alternatively, use forward fill method for another column\n",
    "formatter.fill_missing_values('another_column', method='ffill')\n",
    "\n",
    "print(\"Missing values after filling:\")\n",
    "print(combined_df.isnull().sum())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 2: Standardize the format of any date columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardizing date formats in the 'date_column'\n",
    "formatter.standardize_dates('date_column', date_format='%Y-%m-%d')\n",
    "\n",
    "# Display the transformed data to verify changes\n",
    "print(\"Data after date standardization:\")\n",
    "print(combined_df[['date_column']].head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 3: Normalize numeric columns and convert categorical columns to a standard format.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalizing numeric columns 'sales' and 'profit'\n",
    "numeric_columns = ['sales', 'profit']\n",
    "formatter.normalize_numeric(numeric_columns)\n",
    "\n",
    "# Checking the normalized data\n",
    "print(\"Normalized numeric data:\")\n",
    "print(combined_df[numeric_columns].describe())\n",
    "\n",
    "# Categorizing columns 'category1' and 'category2'\n",
    "category_columns = ['category1', 'category2']\n",
    "formatter.categorize_columns(category_columns)\n",
    "\n",
    "# Checking the data types to verify the categorization\n",
    "print(\"Data types after categorization:\")\n",
    "print(combined_df.dtypes)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 4: Create a new column based on a custom transformation logic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Applying a custom transformation to create a new column 'sales_squared'\n",
    "formatter.custom_transform('sales', lambda x: x ** 2)\n",
    "\n",
    "# Rename the transformed column to 'sales_squared'\n",
    "combined_df.rename(columns={'sales': 'sales_squared'}, inplace=True)\n",
    "\n",
    "# Display the first few rows to verify the transformation\n",
    "print(\"Data with custom transformation:\")\n",
    "print(combined_df[['sales_squared']].head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Challenge: Automate the data cleaning process for future data imports.\n",
    "\n",
    "To automate the data cleaning process, you can define a function that performs all the above steps. This function can be reused whenever you import new data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def clean_and_transform_data(df):\n",
    "    # Initialize the Data Formatter\n",
    "    formatter = DataFormatter(df)\n",
    "\n",
    "    # Fill missing values\n",
    "    formatter.fill_missing_values('column_with_missing_data', fill_value=0)\n",
    "    formatter.fill_missing_values('another_column', method='ffill')\n",
    "\n",
    "    # Standardize date formats\n",
    "    formatter.standardize_dates('date_column', date_format='%Y-%m-%d')\n",
    "\n",
    "    # Normalize numeric columns\n",
    "    numeric_columns = ['sales', 'profit']\n",
    "    formatter.normalize_numeric(numeric_columns)\n",
    "\n",
    "    # Categorize columns\n",
    "    category_columns = ['category1', 'category2']\n",
    "    formatter.categorize_columns(category_columns)\n",
    "\n",
    "    # Apply custom transformation\n",
    "    formatter.custom_transform('sales', lambda x: x ** 2)\n",
    "    df.rename(columns={'sales': 'sales_squared'}, inplace=True)\n",
    "\n",
    "    return df\n",
    "\n",
    "# Apply the cleaning and transformation function to the combined dataframe\n",
    "cleaned_combined_df = clean_and_transform_data(combined_df)\n",
    "\n",
    "# Display the cleaned and transformed data\n",
    "print(\"Cleaned and Transformed Data:\")\n",
    "print(cleaned_combined_df.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This completes Exercise 2. By following these steps, you have successfully cleaned and transformed the integrated dataset using the DataAnalysisToolkit.\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Exercise 3: Handling Large and Complex Datasets\n",
    "- **Objective**: Work with a larger and more complex dataset of your choice (e.g., a dataset from Kaggle or a public API).\n",
    "- **Tasks**:\n",
    "  - Import the dataset using the appropriate connector(s).\n",
    "  - Explore different integration techniques to handle large datasets efficiently.\n",
    "  - Perform advanced data formatting and transformation tasks tailored to the dataset's specifics.\n",
    "- **Challenge**: Optimize the data import process for speed and memory efficiency, especially if dealing with very large datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this exercise, we'll focus on importing a large dataset, integrating it efficiently, and performing advanced data formatting and transformation tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3: Handling Large and Complex Datasets\n",
    "\n",
    "**Objective:** Work with a larger and more complex dataset of your choice (e.g., a dataset from Kaggle or a public API).\n",
    "\n",
    "#### Step 1: Import the dataset using the appropriate connector(s).\n",
    "\n",
    "For this example, let's assume we're working with a large CSV file, a SQL database, and a web API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing data from a large CSV file\n",
    "csv_connector = ExcelConnector('path/to/large_dataset.csv')\n",
    "df_csv = csv_connector.load_data()\n",
    "\n",
    "print(\"Data from CSV file:\")\n",
    "print(df_csv.head())\n",
    "\n",
    "# Importing data from a SQL database\n",
    "# Replace with your actual database URI and query\n",
    "sql_connector = SQLConnector('postgresql://username:password@localhost:5432/mydatabase')\n",
    "query = \"SELECT * FROM large_table\"\n",
    "df_sql_large = sql_connector.query_data(query)\n",
    "\n",
    "print(\"Data from SQL Database:\")\n",
    "print(df_sql_large.head())\n",
    "\n",
    "# Importing data from a web API\n",
    "# Replace with the actual API base URL and authentication credentials\n",
    "api_connector = APIConnector('https://api.example.com', auth=('username', 'password'))\n",
    "endpoint = 'large_data_endpoint'\n",
    "response_large = api_connector.get(endpoint)\n",
    "\n",
    "# Assuming the response is JSON and converting it to a DataFrame\n",
    "df_api_large = pd.DataFrame(response_large.json())\n",
    "\n",
    "print(\"Data from API:\")\n",
    "print(df_api_large.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 2: Explore different integration techniques to handle large datasets efficiently.\n",
    "\n",
    "For large datasets, it's often necessary to use efficient data handling techniques, such as chunking and multiprocessing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the Data Integrator\n",
    "integrator = DataIntegrator()\n",
    "\n",
    "# Adding dataframes to the integrator\n",
    "integrator.add_data(df_csv)\n",
    "integrator.add_data(df_sql_large)\n",
    "integrator.add_data(df_api_large)\n",
    "\n",
    "# Assuming we want to concatenate the dataframes\n",
    "combined_large_df = integrator.concatenate_data()\n",
    "print(\"Combined Data:\")\n",
    "print(combined_large_df.head())\n",
    "\n",
    "# Alternatively, if you need to merge the dataframes on a common key:\n",
    "# combined_large_df = integrator.merge_data(on='common_key')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 3: Perform advanced data formatting and transformation tasks tailored to the dataset's specifics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the Data Formatter\n",
    "formatter_large = DataFormatter(combined_large_df)\n",
    "\n",
    "# Standardize date formats in the 'date_column'\n",
    "formatter_large.standardize_dates('date_column', date_format='%Y-%m-%d')\n",
    "\n",
    "# Normalize numeric columns 'sales' and 'profit'\n",
    "numeric_columns_large = ['sales', 'profit']\n",
    "formatter_large.normalize_numeric(numeric_columns_large)\n",
    "\n",
    "# Categorize columns 'category1' and 'category2'\n",
    "category_columns_large = ['category1', 'category2']\n",
    "formatter_large.categorize_columns(category_columns_large)\n",
    "\n",
    "# Fill missing values\n",
    "formatter_large.fill_missing_values('column_with_missing_data', fill_value=0)\n",
    "formatter_large.fill_missing_values('another_column', method='ffill')\n",
    "\n",
    "# Apply custom transformation to create a new column 'sales_squared'\n",
    "formatter_large.custom_transform('sales', lambda x: x ** 2)\n",
    "combined_large_df.rename(columns={'sales': 'sales_squared'}, inplace=True)\n",
    "\n",
    "# Display the cleaned and transformed data\n",
    "print(\"Cleaned and Transformed Combined Data:\")\n",
    "print(combined_large_df.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Challenge: Optimize the data import process for speed and memory efficiency.\n",
    "\n",
    "To handle large datasets efficiently, consider using chunking and multiprocessing techniques.\n",
    "\n",
    "**Using Chunking for Large CSV Files:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using chunking to load large CSV file in smaller parts\n",
    "chunk_size = 10000\n",
    "chunks = []\n",
    "\n",
    "for chunk in pd.read_csv('path/to/large_dataset.csv', chunksize=chunk_size):\n",
    "    chunks.append(chunk)\n",
    "\n",
    "df_csv_large = pd.concat(chunks, axis=0)\n",
    "\n",
    "print(\"Data from CSV file (using chunking):\")\n",
    "print(df_csv_large.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Using Multiprocessing for Parallel Data Processing:**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Integrating the custom connector with DataAnalysisToolkit\n",
    "from dataanalysistoolkit.integrators import DataIntegrator\n",
    "\n",
    "# Initialize the Data Integrator\n",
    "integrator = DataIntegrator()\n",
    "\n",
    "# Adding the binary data to the integrator\n",
    "integrator.add_data(df_binary)\n",
    "\n",
    "# Assuming we also have data from an Excel file and a SQL database\n",
    "integrator.add_data(df_excel)\n",
    "integrator.add_data(df_sql)\n",
    "\n",
    "# Concatenating the dataframes\n",
    "combined_df = integrator.concatenate_data()\n",
    "print(\"Combined Data with Custom Connector:\")\n",
    "print(combined_df.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This completes Exercise 3. You have successfully handled large and complex datasets, imported them efficiently, and performed advanced data formatting and transformation tasks using the DataAnalysisToolkit.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Exercise 4: Customizing the Data Import Process\n",
    "- **Objective**: Extend or customize the DataAnalysisToolkit to suit a unique data import requirement.\n",
    "- **Tasks**:\n",
    "  - Identify a specific need or limitation in the current data import process.\n",
    "  - Modify an existing connector or create a new one to address this need.\n",
    "  - Test your custom solution with relevant data sources.\n",
    "- **Challenge**: Ensure that your custom solution is robust, handles errors gracefully, and integrates well with the rest of the toolkit."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This exercise will involve identifying a specific need or limitation in the current data import process, modifying an existing connector, or creating a new one.\n",
    "\n",
    "### Exercise 4: Customizing the Data Import Process\n",
    "\n",
    "**Objective:** Extend or customize the DataAnalysisToolkit to suit a unique data import requirement.\n",
    "\n",
    "#### Step 1: Identify a specific need or limitation in the current data import process.\n",
    "\n",
    "Let's assume that we have a unique data source that requires special handling. For example, we might need to import data from a custom binary file format that isn't supported by the existing connectors in the DataAnalysisToolkit.\n",
    "\n",
    "#### Step 2: Modify an existing connector or create a new one to address this need.\n",
    "\n",
    "We'll create a custom connector class to handle our unique binary file format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom connector for binary file format\n",
    "import struct\n",
    "\n",
    "class BinaryFileConnector:\n",
    "    def __init__(self, file_path):\n",
    "        self.file_path = file_path\n",
    "\n",
    "    def load_data(self):\n",
    "        data = []\n",
    "        with open(self.file_path, 'rb') as file:\n",
    "            while True:\n",
    "                chunk = file.read(16)  # Assuming each record is 16 bytes long\n",
    "                if not chunk:\n",
    "                    break\n",
    "                record = struct.unpack('4f', chunk)  # Unpack 4 floats from each chunk\n",
    "                data.append(record)\n",
    "        return pd.DataFrame(data, columns=['col1', 'col2', 'col3', 'col4'])\n",
    "\n",
    "# Using the custom connector to load data from a binary file\n",
    "binary_connector = BinaryFileConnector('/mnt/data/custom_data.bin')\n",
    "df_binary = binary_connector.load_data()\n",
    "\n",
    "print(\"Data from custom binary file:\")\n",
    "print(df_binary.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 3: Test your custom solution with relevant data sources.\n",
    "\n",
    "We can now test our custom connector by loading data from a binary file and performing some basic operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing the custom connector\n",
    "binary_connector = BinaryFileConnector('/mnt/data/custom_data.bin')\n",
    "df_binary = binary_connector.load_data()\n",
    "\n",
    "# Displaying the first few rows of the loaded data\n",
    "print(\"Data from custom binary file:\")\n",
    "print(df_binary.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 4: Ensure that your custom solution is robust, handles errors gracefully, and integrates well with the rest of the toolkit.\n",
    "\n",
    "We can improve our custom connector by adding error handling and making it more robust."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BinaryFileConnector:\n",
    "    def __init__(self, file_path):\n",
    "        self.file_path = file_path\n",
    "\n",
    "    def load_data(self):\n",
    "        data = []\n",
    "        try:\n",
    "            with open(self.file_path, 'rb') as file:\n",
    "                while True:\n",
    "                    chunk = file.read(16)  # Assuming each record is 16 bytes long\n",
    "                    if not chunk:\n",
    "                        break\n",
    "                    record = struct.unpack('4f', chunk)  # Unpack 4 floats from each chunk\n",
    "                    data.append(record)\n",
    "            return pd.DataFrame(data, columns=['col1', 'col2', 'col3', 'col4'])\n",
    "        except FileNotFoundError:\n",
    "            print(f\"Error: The file {self.file_path} was not found.\")\n",
    "            return pd.DataFrame()\n",
    "        except struct.error:\n",
    "            print(f\"Error: Could not unpack data from file {self.file_path}.\")\n",
    "            return pd.DataFrame()\n",
    "\n",
    "# Using the custom connector to load data from a binary file\n",
    "binary_connector = BinaryFileConnector('/mnt/data/custom_data.bin')\n",
    "df_binary = binary_connector.load_data()\n",
    "\n",
    "print(\"Data from custom binary file with error handling:\")\n",
    "print(df_binary.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With these modifications, our custom connector is now more robust and can handle errors gracefully.\n",
    "\n",
    "### Challenge: Integrate the custom connector with the DataAnalysisToolkit's existing functionality.\n",
    "\n",
    "To fully integrate our custom connector with the DataAnalysisToolkit, we can add it to the toolkit's data source module and use it seamlessly with other connectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Integrating the custom connector with DataAnalysisToolkit\n",
    "from dataanalysistoolkit.integrators import DataIntegrator\n",
    "\n",
    "# Initialize the Data Integrator\n",
    "integrator = DataIntegrator()\n",
    "\n",
    "# Adding the binary data to the integrator\n",
    "integrator.add_data(df_binary)\n",
    "\n",
    "# Assuming we also have data from an Excel file and a SQL database\n",
    "integrator.add_data(df_excel)\n",
    "integrator.add_data(df_sql)\n",
    "\n",
    "# Concatenating the dataframes\n",
    "combined_df = integrator.concatenate_data()\n",
    "print(\"Combined Data with Custom Connector:\")\n",
    "print(combined_df.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This completes Exercise 4. You have successfully customized the DataAnalysisToolkit to handle a unique data import requirement by creating a custom connector, ensuring it is robust, and integrating it with the existing functionality of the toolkit."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Exercise 5: Real-world Application\n",
    "- **Objective**: Apply the DataAnalysisToolkit to a real-world data analysis project.\n",
    "- **Tasks**:\n",
    "  - Identify a real-world problem that can be addressed through data analysis.\n",
    "  - Collect and import data from relevant sources using the toolkit.\n",
    "  - Clean, transform, and integrate the data in preparation for analysis.\n",
    "- **Challenge**: Provide insights, visualizations, or a predictive model based on the integrated dataset.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ideas for Real-world Applications\n",
    "\n",
    "1. **Sales Data Analysis:**\n",
    "   - **Objective:** Analyze sales data to identify trends, seasonality, and top-selling products.\n",
    "   - **Data Sources:**\n",
    "     - Excel files with monthly sales data.\n",
    "     - SQL database with detailed sales transactions.\n",
    "     - API providing real-time sales updates.\n",
    "   - **Steps:**\n",
    "     - Import and integrate data from Excel, SQL, and API.\n",
    "     - Clean and transform the data.\n",
    "     - Perform trend analysis and seasonal decomposition.\n",
    "     - Identify top-selling products and regions.\n",
    "     - Generate visualizations for management reports.\n",
    "\n",
    "2. **Customer Segmentation:**\n",
    "   - **Objective:** Segment customers based on their purchasing behavior and demographics.\n",
    "   - **Data Sources:**\n",
    "     - CSV file with customer demographics.\n",
    "     - SQL database with transaction history.\n",
    "     - API with social media engagement data.\n",
    "   - **Steps:**\n",
    "     - Import and integrate customer demographics, transaction history, and social media data.\n",
    "     - Clean and preprocess the data.\n",
    "     - Apply clustering algorithms to segment customers.\n",
    "     - Analyze and interpret customer segments.\n",
    "     - Create targeted marketing strategies for each segment.\n",
    "\n",
    "3. **Financial Portfolio Analysis:**\n",
    "   - **Objective:** Analyze and optimize a financial portfolio.\n",
    "   - **Data Sources:**\n",
    "     - Excel files with historical stock prices and portfolio details.\n",
    "     - SQL database with transaction history.\n",
    "     - API providing real-time financial news and stock data.\n",
    "   - **Steps:**\n",
    "     - Import and integrate historical prices, portfolio details, and real-time data.\n",
    "     - Clean and transform the data.\n",
    "     - Calculate key financial metrics (e.g., ROI, Sharpe ratio).\n",
    "     - Perform risk assessment and optimization.\n",
    "     - Generate reports and visualizations for portfolio performance.\n",
    "\n",
    "4. **Health Data Analysis:**\n",
    "   - **Objective:** Analyze patient health records to identify common health issues and trends.\n",
    "   - **Data Sources:**\n",
    "     - Excel files with patient records and clinical trial results.\n",
    "     - SQL database with hospital data.\n",
    "     - API providing public health data.\n",
    "   - **Steps:**\n",
    "     - Import and integrate patient records, clinical trial data, and public health data.\n",
    "     - Clean and preprocess the data.\n",
    "     - Perform statistical analysis to identify common health issues.\n",
    "     - Analyze trends and correlations in the data.\n",
    "     - Generate visualizations for health reports.\n",
    "\n",
    "5. **E-commerce Data Analysis:**\n",
    "   - **Objective:** Improve website performance and user experience based on e-commerce data.\n",
    "   - **Data Sources:**\n",
    "     - CSV files with user behavior data.\n",
    "     - SQL database with product information and sales data.\n",
    "     - API providing real-time user feedback.\n",
    "   - **Steps:**\n",
    "     - Import and integrate user behavior, product information, and feedback data.\n",
    "     - Clean and transform the data.\n",
    "     - Analyze user behavior and identify patterns.\n",
    "     - Perform A/B testing on website changes.\n",
    "     - Generate reports and recommendations for website optimization.\n",
    "\n",
    "6. **Environmental Data Analysis:**\n",
    "   - **Objective:** Monitor and analyze environmental data to track pollution levels.\n",
    "   - **Data Sources:**\n",
    "     - CSV files with historical pollution data.\n",
    "     - SQL database with weather data.\n",
    "     - API providing real-time pollution levels.\n",
    "   - **Steps:**\n",
    "     - Import and integrate historical pollution data, weather data, and real-time pollution levels.\n",
    "     - Clean and preprocess the data.\n",
    "     - Perform time-series analysis to identify trends.\n",
    "     - Analyze the impact of weather on pollution levels.\n",
    "     - Generate visualizations and reports for environmental monitoring.\n",
    "\n",
    "7. **Social Media Sentiment Analysis:**\n",
    "   - **Objective:** Analyze social media sentiment about a brand or product.\n",
    "   - **Data Sources:**\n",
    "     - CSV files with historical social media posts.\n",
    "     - SQL database with customer feedback.\n",
    "     - API providing real-time social media data.\n",
    "   - **Steps:**\n",
    "     - Import and integrate social media posts, customer feedback, and real-time data.\n",
    "     - Clean and preprocess the data.\n",
    "     - Perform sentiment analysis using natural language processing (NLP) techniques.\n",
    "     - Identify key topics and trends in social media discussions.\n",
    "     - Generate reports and visualizations for brand sentiment.\n",
    "\n",
    "8. **Real Estate Market Analysis:**\n",
    "   - **Objective:** Analyze real estate market trends and property values.\n",
    "   - **Data Sources:**\n",
    "     - Excel files with historical property sales data.\n",
    "     - SQL database with property details.\n",
    "     - API providing real-time market listings.\n",
    "   - **Steps:**\n",
    "     - Import and integrate historical sales data, property details, and real-time listings.\n",
    "     - Clean and transform the data.\n",
    "     - Perform market trend analysis and price forecasting.\n",
    "     - Identify key factors affecting property values.\n",
    "     - Generate reports and visualizations for market insights.\n",
    "\n",
    "9. **Supply Chain Optimization:**\n",
    "   - **Objective:** Optimize the supply chain process to reduce costs and improve efficiency.\n",
    "   - **Data Sources:**\n",
    "     - CSV files with supply chain data (inventory, shipments).\n",
    "     - SQL database with supplier information.\n",
    "     - API providing real-time logistics data.\n",
    "   - **Steps:**\n",
    "     - Import and integrate supply chain data, supplier information, and logistics data.\n",
    "     - Clean and preprocess the data.\n",
    "     - Perform analysis to identify bottlenecks and inefficiencies.\n",
    "     - Optimize inventory levels and shipment schedules.\n",
    "     - Generate reports and recommendations for supply chain improvements.\n",
    "\n",
    "10. **Educational Data Analysis:**\n",
    "    - **Objective:** Analyze student performance data to improve educational outcomes.\n",
    "    - **Data Sources:**\n",
    "      - Excel files with student grades and attendance records.\n",
    "      - SQL database with course details.\n",
    "      - API providing real-time educational resources and feedback.\n",
    "    - **Steps:**\n",
    "      - Import and integrate student performance data, course details, and real-time resources.\n",
    "      - Clean and preprocess the data.\n",
    "      - Perform analysis to identify factors affecting student performance.\n",
    "      - Develop predictive models for student success.\n",
    "      - Generate reports and recommendations for educational improvements.\n",
    "\n",
    "These ideas provide a broad range of real-world applications for the DataAnalysisToolkit, covering various domains and data sources. By following these steps, you can effectively utilize the toolkit to import, integrate, clean, and analyze data to gain valuable insights and make data-driven decisions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
